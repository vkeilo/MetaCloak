{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 在SD21上测试PAN攻击"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import wandb\n",
    "import argparse\n",
    "import copy\n",
    "import hashlib\n",
    "import itertools\n",
    "import logging\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "import sys\n",
    "sys.path.append(\"/data/home/yekai/github/mypro/anotherMetacloak\")\n",
    "from pathlib import Path\n",
    "import datasets\n",
    "import diffusers\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.checkpoint\n",
    "import transformers\n",
    "from accelerate import Accelerator\n",
    "from accelerate.logging import get_logger\n",
    "from accelerate.utils import set_seed\n",
    "from diffusers import AutoencoderKL, DDPMScheduler, DiffusionPipeline, UNet2DConditionModel\n",
    "from diffusers.utils.import_utils import is_xformers_available\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer, PretrainedConfig\n",
    "from robust_facecloak.model.db_train import  DreamBoothDatasetFromTensor\n",
    "from robust_facecloak.model.db_train import import_model_class_from_model_name_or_path\n",
    "from robust_facecloak.generic.data_utils import PromptDataset, load_data\n",
    "from robust_facecloak.generic.share_args import share_parse_args\n",
    "\n",
    "import pickle\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class myargs():\n",
    "    def __init__(self):\n",
    "        self.learning_rate=5e-7\n",
    "        self.total_trail_num = 4\n",
    "        self.instance_prompt=\"a photo of sks person\"\n",
    "        self.class_data_dir=\"/data/home/yekai/github/mypro/anotherMetacloak/prior-data/SD21base/class-person\"\n",
    "        self.instance_data_dir_for_adversarial = \"/data/home/yekai/github/mypro/anotherMetacloak/dataset/VGGFace2-clean/0/set_B\"\n",
    "        self.output_dir = \"./tmpdata\"\n",
    "        self.class_prompt=\"a photo of a person\"\n",
    "        self.total_train_steps = 6\n",
    "        self.interval = 2\n",
    "        self.advance_steps = 2\n",
    "        self.radius = 11\n",
    "        self.resolution=512\n",
    "        self.center_crop=True\n",
    "        self.with_prior_preservation=True\n",
    "        self.revision = None\n",
    "        self.prior_loss_weight = 1.0\n",
    "        self.train_text_encoder = True\n",
    "        self.enable_xformers_memory_efficient_attention = True\n",
    "        self.mixed_precision = \"bf16\"\n",
    "        self.attack_pgd_random_start = False\n",
    "        \n",
    "\n",
    "args = myargs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 首先是模型加载和训练代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_few_step(\n",
    "    args,\n",
    "    models,\n",
    "    tokenizer,\n",
    "    noise_scheduler,\n",
    "    vae,\n",
    "    data_tensor: torch.Tensor,\n",
    "    num_steps=20,\n",
    "    step_wise_save=False,\n",
    "    save_step=100, \n",
    "    retain_graph=False,\n",
    "    task_loss_name = None,\n",
    "    copy_flag = True\n",
    "):\n",
    "    # Load the tokenizer\n",
    "    if copy_flag:\n",
    "        unet, text_encoder = copy.deepcopy(models[0]), copy.deepcopy(models[1])\n",
    "    else:\n",
    "        unet, text_encoder = models[0], models[1]\n",
    "    # 绑定unet和文本编码器的参数，共同优化\n",
    "    params_to_optimize = itertools.chain(unet.parameters(), text_encoder.parameters())\n",
    "\n",
    "    # 设置优化器，优化目标为unet参数和文本编码器参数\n",
    "    optimizer = torch.optim.AdamW(\n",
    "        params_to_optimize,\n",
    "        lr=args.learning_rate,\n",
    "        betas=(0.9, 0.999),\n",
    "        weight_decay=1e-2,\n",
    "        eps=1e-08,\n",
    "    )\n",
    "\n",
    "    train_dataset = DreamBoothDatasetFromTensor(\n",
    "        data_tensor,\n",
    "        # A photo of sks person\n",
    "        args.instance_prompt,\n",
    "        tokenizer,\n",
    "        args.class_data_dir,\n",
    "        args.class_prompt,\n",
    "        args.resolution,\n",
    "        args.center_crop,\n",
    "    )\n",
    "\n",
    "    weight_dtype = torch.bfloat16\n",
    "    device = torch.device(\"cuda\")\n",
    "\n",
    "    # 将关键模型移动到对应设备\n",
    "    vae.to(device, dtype=weight_dtype)\n",
    "    text_encoder.to(device, dtype=weight_dtype)\n",
    "    unet.to(device, dtype=weight_dtype)\n",
    "\n",
    "    \n",
    "    step2modelstate={}\n",
    "        \n",
    "    pbar = tqdm(total=num_steps, desc=\"training\")\n",
    "    for step in range(num_steps):\n",
    "        # print(calculate_model_hash(text_encoder))\n",
    "        # 根据设置选择是否保存训练中间过程参数\n",
    "        if step_wise_save and ((step+1) % save_step == 0 or step == 0):\n",
    "            # make sure the model state dict is put to cpu\n",
    "            step2modelstate[step] = {\n",
    "                \"unet\": copy.deepcopy(unet.cpu().state_dict()),\n",
    "                \"text_encoder\": copy.deepcopy(text_encoder.cpu().state_dict()),\n",
    "            }\n",
    "            # move the model back to gpu\n",
    "            unet.to(device, dtype=weight_dtype); text_encoder.to(device, dtype=weight_dtype)\n",
    "            \n",
    "        pbar.update(1)\n",
    "        # 训练模式\n",
    "        unet.train()\n",
    "        text_encoder.train()\n",
    "        # 循环从训练数据集中取一个样本\n",
    "        step_data = train_dataset[step % len(train_dataset)]\n",
    "        # 将样本中的类别图片和实例图片整合并移动到设备上\n",
    "        # print((step_data[\"instance_images\"]))\n",
    "        # print((step_data[\"class_images\"]))\n",
    "        pixel_values = torch.stack([step_data[\"instance_images\"].to(device), step_data[\"class_images\"].to(device)]).to(\n",
    "            device, dtype=weight_dtype\n",
    "        )\n",
    "        # 将样本中的类别提示词和实例提示词整合并移动到设备上\n",
    "        input_ids = torch.cat([step_data[\"instance_prompt_ids\"], step_data[\"class_prompt_ids\"]], dim=0).to(device)\n",
    "        # 使用VAE对图像进行编码，并对潜在表示进行后处理\n",
    "        latents = vae.encode(pixel_values).latent_dist.sample()\n",
    "        latents = latents * vae.config.scaling_factor\n",
    "\n",
    "        # Sample noise that we'll add to the latents\n",
    "        # 向图片编码向量（潜在空间向量表示）添加随机噪声\n",
    "        noise = torch.randn_like(latents)\n",
    "        # batch_size\n",
    "        bsz = latents.shape[0]\n",
    "        # Sample a random timestep for each image\n",
    "        # 为每个图片生成一个随机step\n",
    "        timesteps = torch.randint(0, noise_scheduler.config.num_train_timesteps, (bsz,), device=latents.device)\n",
    "        timesteps = timesteps.long()\n",
    "\n",
    "        # Add noise to the latents according to the noise magnitude at each timestep\n",
    "        # (this is the forward diffusion process)\n",
    "        # 前向过程，得到前向扩散特定时间步后的图片的潜在空间向量\n",
    "        noisy_latents = noise_scheduler.add_noise(latents, noise, timesteps)\n",
    "\n",
    "        # Get the text embedding for conditioning\n",
    "        # 文本编码向量作为条件信息\n",
    "        encoder_hidden_states = text_encoder(input_ids)[0]\n",
    "        \n",
    "        # Predict the noise residual\n",
    "        # 模型基于当前的噪声潜在表示（noisy_latents）、时间步（timesteps）和文本条件（encoder_hidden_states），预测噪声残差\n",
    "        model_pred = unet(noisy_latents, timesteps, encoder_hidden_states).sample\n",
    "\n",
    "        # Get the target for loss depending on the prediction type\n",
    "        # 预测的可以是噪声，也可以是变化速度\n",
    "        if noise_scheduler.config.prediction_type == \"epsilon\":\n",
    "            target = noise\n",
    "        elif noise_scheduler.config.prediction_type == \"v_prediction\":\n",
    "            target = noise_scheduler.get_velocity(latents, noise, timesteps)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown prediction type {noise_scheduler.config.prediction_type}\")\n",
    "\n",
    "        # with prior preservation loss\n",
    "        # 可选是否使用先验保留损失\n",
    "        if args.with_prior_preservation:\n",
    "            # 再次分为一半一半，对应之前的stack操作\n",
    "            model_pred, model_pred_prior = torch.chunk(model_pred, 2, dim=0)\n",
    "            target, target_prior = torch.chunk(target, 2, dim=0)\n",
    "\n",
    "            # Compute instance loss\n",
    "            instance_loss = F.mse_loss(model_pred.float(), target.float(), reduction=\"mean\")\n",
    "\n",
    "            # Compute prior loss  确保在原来类别上的生成能力不丢失\n",
    "            prior_loss = F.mse_loss(model_pred_prior.float(), target_prior.float(), reduction=\"mean\")\n",
    "\n",
    "            # Add the prior loss to the instance loss.\n",
    "            loss = instance_loss + args.prior_loss_weight * prior_loss\n",
    "\n",
    "        else:\n",
    "            # 不使用先验保留损失\n",
    "            loss = F.mse_loss(model_pred.float(), target.float(), reduction=\"mean\")\n",
    "        if task_loss_name is not None:\n",
    "            wandb.log({f\"{task_loss_name}\": loss.item()})\n",
    "        # 反向传播\n",
    "        loss.backward(retain_graph=retain_graph)\n",
    "        # 梯度裁剪\n",
    "        torch.nn.utils.clip_grad_norm_(params_to_optimize, 1.0, error_if_nonfinite=True)\n",
    "        # 参数优化\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    pbar.close()\n",
    "    # 返回训练的参数数据\n",
    "    if step_wise_save:\n",
    "        return [unet, text_encoder], step2modelstate\n",
    "    else:     \n",
    "        return [unet, text_encoder]\n",
    "\n",
    "# 主要模型的加载\n",
    "def load_model(args, model_path):\n",
    "    print(model_path)\n",
    "    # import correct text encoder class\n",
    "    text_encoder_cls = import_model_class_from_model_name_or_path(model_path, args.revision)\n",
    "\n",
    "    # Load scheduler and models\n",
    "    # 文本编码器加载\n",
    "    text_encoder = text_encoder_cls.from_pretrained(\n",
    "        model_path,\n",
    "        subfolder=\"text_encoder\",\n",
    "        revision=args.revision,\n",
    "    )\n",
    "    # unet加载\n",
    "    unet = UNet2DConditionModel.from_pretrained(model_path, subfolder=\"unet\", revision=args.revision)\n",
    "    # tokenizer加载\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        model_path,\n",
    "        subfolder=\"tokenizer\",\n",
    "        revision=args.revision,\n",
    "        use_fast=False,\n",
    "    )\n",
    "    # 使用DDPM同款调度器\n",
    "    noise_scheduler = DDPMScheduler.from_pretrained(model_path, subfolder=\"scheduler\")\n",
    "    # 加载预训练的vae，vae不需要更新参数\n",
    "    vae = AutoencoderKL.from_pretrained(model_path, subfolder=\"vae\", revision=args.revision)\n",
    "\n",
    "    vae.requires_grad_(False)\n",
    "\n",
    "    # 甚至可以不更新文本编码器的参数\n",
    "    if not args.train_text_encoder:\n",
    "        text_encoder.requires_grad_(False)\n",
    "\n",
    "    if args.enable_xformers_memory_efficient_attention:\n",
    "        print(\"You selected to used efficient xformers\")\n",
    "        print(\"Make sure to install the following packages before continue\")\n",
    "        print(\"pip install triton==2.0.0.dev20221031\")\n",
    "        print(\"pip install pip install xformers==0.0.17.dev461\")\n",
    "\n",
    "        unet.enable_xformers_memory_efficient_attention()\n",
    "    # 返回5个关键模型\n",
    "    return text_encoder, unet, tokenizer, noise_scheduler, vae\n",
    "\n",
    "def save_image(perturbed_data, id_stamp):\n",
    "    save_folder = f\"{args.output_dir}/noise-ckpt/{id_stamp}\"\n",
    "    os.makedirs(save_folder, exist_ok=True)\n",
    "    noised_imgs = perturbed_data.detach()\n",
    "    img_names = [\n",
    "        str(instance_path).split(\"/\")[-1]\n",
    "        for instance_path in list(Path(args.instance_data_dir_for_adversarial).iterdir())\n",
    "    ]\n",
    "    for img_pixel, img_name in zip(noised_imgs, img_names):\n",
    "        save_path = os.path.join(save_folder, f\"noisy_{img_name}\")\n",
    "        Image.fromarray(\n",
    "            img_pixel.float().detach().cpu().permute(1, 2, 0).numpy().squeeze().astype(np.uint8)\n",
    "        ).save(save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 加载模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_paths = [\"/data/home/yekai/github/mypro/anotherMetacloak/SD/stable-diffusion-2-1-base\"]\n",
    "num_models = len(model_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_BANKS = [load_model(args, path) for path in model_paths]\n",
    "MODEL_STATEDICTS = [\n",
    "    {\n",
    "        \"text_encoder\": MODEL_BANKS[i][0].state_dict(),\n",
    "        \"unet\": MODEL_BANKS[i][1].state_dict(),\n",
    "    }\n",
    "    for i in range(num_models)\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 开始初始的预训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载原始扰动数据\n",
    "perturbed_data = load_data(\n",
    "    args.instance_data_dir_for_adversarial,\n",
    "    # size=args.resolution,\n",
    "    # center_crop=args.center_crop,\n",
    ")\n",
    "original_data= copy.deepcopy(perturbed_data)\n",
    "\n",
    "\n",
    "init_model_state_pool = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pbar = tqdm(total=num_models, desc=\"initializing models\")\n",
    "# # split sub-models\n",
    "# # 对于每一个模型，都进行一次训练\n",
    "# for j in range(num_models):\n",
    "#     init_model_state_pool[j] = {}\n",
    "#     # 提取关键模块\n",
    "#     text_encoder, unet, tokenizer, noise_scheduler, vae = MODEL_BANKS[j]\n",
    "    \n",
    "#     # 加载unet和text_encoder的模型参数\n",
    "#     unet.load_state_dict(MODEL_STATEDICTS[j][\"unet\"])\n",
    "#     text_encoder.load_state_dict(MODEL_STATEDICTS[j][\"text_encoder\"])\n",
    "#     # 打包unet和text_encoder\n",
    "#     f_ori = [unet, text_encoder]\n",
    "#     # 得到训练total_train_steps步之后的unet, text_encoder参数以及中间状态参数\n",
    "#     f_ori, step2state_dict = train_few_step(\n",
    "#             args,\n",
    "#             f_ori,\n",
    "#             tokenizer,\n",
    "#             noise_scheduler,\n",
    "#             vae,\n",
    "#             perturbed_data.float(),\n",
    "#             args.total_train_steps,\n",
    "#             step_wise_save=True,\n",
    "#             save_step=args.interval,\n",
    "#             task_loss_name=None,\n",
    "#     )  \n",
    "#     # init_model_state_pool就来保存训练中间状态参数\n",
    "#     init_model_state_pool[j] = step2state_dict\n",
    "\n",
    "#     # 释放占用的资源\n",
    "#     del f_ori, unet, text_encoder, tokenizer, noise_scheduler, vae\n",
    "#     import gc\n",
    "#     gc.collect()\n",
    "#     torch.cuda.empty_cache()\n",
    "#     pbar.update(1)\n",
    "# pbar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAVE init_model_state_pool\n",
    "# 定义保存文件的路径\n",
    "filename = \"./tmpdata/init_model_state_pool.pth\"\n",
    "\n",
    "# 使用pickle将数据保存到文件\n",
    "# with open(filename, 'wb') as file:\n",
    "#     pickle.dump(init_model_state_pool, file)\n",
    "\n",
    "# 读取保存的文件\n",
    "with open(filename, 'rb') as f:\n",
    "    init_model_state_pool = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(init_model_state_pool)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 优化扰动"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 使用求解器的结果"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 使用判别器的结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 该版本无显存溢出问题，但是需要21G显存\n",
    "device = torch.device('cuda')\n",
    "weight_dtype = torch.bfloat16\n",
    "if args.mixed_precision == \"fp32\":\n",
    "    weight_dtype = torch.float32\n",
    "elif args.mixed_precision == \"fp16\":\n",
    "    weight_dtype = torch.float16\n",
    "elif args.mixed_precision == \"bf16\":\n",
    "    weight_dtype = torch.bfloat16\n",
    "class PAN_attacker():\n",
    "    def __init__(self, lambda_D=0.1, lambda_S=10, omiga=0.5, alpha=1/255, k=2, radius=11, x_range=[0,255], steps=1, mode = \"D\", use_val = \"last\", no_attack = False):\n",
    "        self.lambda_D = lambda_D\n",
    "        self.lambda_S = lambda_S\n",
    "        self.omiga = omiga\n",
    "        self.alpha = alpha\n",
    "        self.k = k\n",
    "        self.radius = radius\n",
    "        self.random_start = args.attack_pgd_random_start\n",
    "        self.weight_dtype = torch.bfloat16  # 默认类型\n",
    "        self.left = x_range[0]\n",
    "        self.right = x_range[1]\n",
    "        self.norm_type = 'l-infty'\n",
    "        self.steps = steps\n",
    "        self.mode = mode\n",
    "        self.use_val = use_val\n",
    "        self.noattack = no_attack\n",
    "        if args.mixed_precision == \"fp32\":\n",
    "            self.weight_dtype = torch.float32\n",
    "        elif args.mixed_precision == \"fp16\":\n",
    "            self.weight_dtype = torch.float16\n",
    "        elif args.mixed_precision == \"bf16\":\n",
    "            self.weight_dtype = torch.bfloat16\n",
    "        \n",
    "    def attack(self, f, perturbed_data, ori_image, vae, tokenizer, noise_scheduler):\n",
    "        if self.noattack:\n",
    "            print(\"defender no need to defend\")\n",
    "            return perturbed_data, 0\n",
    "\n",
    "\n",
    "        f = [f[0].to(device, dtype=self.weight_dtype), f[1].to(device, dtype=self.weight_dtype)]\n",
    "        vae.to(device, dtype=self.weight_dtype)\n",
    "        perturbed_data = perturbed_data.to(device)\n",
    "        # ori_image = deepcopy(perturbed_data).to(device)\n",
    "        ori_image = ori_image.to(device)\n",
    "        # batch_size = ori_image.size(0)\n",
    "        # random start部分操作逻辑未设计\n",
    "        if self.random_start:\n",
    "            r=self.radius\n",
    "            initial_pertubations = torch.zeros_like(ori_image).uniform_(-r, r).to(device)\n",
    "            adv_image = perturbed_data+initial_pertubations\n",
    "            perturbed_data = adv_image - self._clip_(adv_image, ori_image, mode=\"D\")\n",
    "        else:\n",
    "            initial_pertubations = torch.zeros_like(ori_image).to(device)\n",
    "        # 此轮攻击的初始扰动都是0\n",
    "        pertubation_data_D = deepcopy(perturbed_data)\n",
    "        pertubation_data_S = deepcopy(perturbed_data)\n",
    "        best_loss_S = float('inf')\n",
    "        best_loss_D = float('inf')\n",
    "        best_pertubation_data_S = deepcopy(perturbed_data)\n",
    "        best_pertubation_data_D = deepcopy(perturbed_data)\n",
    "\n",
    "        for i in range(self.steps):\n",
    "            # print(f'step {i} :per_s is {pertubations_S[0]}')\n",
    "            # 更新扰动D\n",
    "            pertubation_data_D, loss_D = self.update_pertubation_data_D(f, pertubation_data_D, ori_image, vae, tokenizer, noise_scheduler)\n",
    "            if loss_D < best_loss_D:\n",
    "                best_loss_D = loss_D\n",
    "                # if mode == \"D\":\n",
    "                    # print(f'pertubation_D, max val is {self.get_Linfty_norm(pertubations_D)}')\n",
    "                    # print(f\"find a better pertubation , max val is {self.get_Linfty_norm( pertubations_D.to('cpu') + perturbed_data.to('cpu') - ori_image.to('cpu') )}\")\n",
    "                best_pertubation_data_D = deepcopy(pertubation_data_D)\n",
    "            # 更新扰动S\n",
    "            pertubation_data_S, loss_S = self.update_pertubation_S(f, pertubation_data_S, pertubation_data_D, ori_image, vae, tokenizer, noise_scheduler)\n",
    "            print(f'epoch: {i}, loss_S: {loss_S:.4f}, loss_D: {loss_D: .4f}')\n",
    "            if loss_S < best_loss_S:\n",
    "                best_loss_S = loss_S\n",
    "                # if mode == \"S\":\n",
    "                    # print(f\"find a better pertubation , max val is {self.get_Linfty_norm(pertubations_S.to('cpu') + perturbed_data.to('cpu') - ori_image.to('cpu'))}\")\n",
    "                best_pertubation_data_S = deepcopy(pertubation_data_S)\n",
    "        \n",
    "        assert self.mode in [\"S\", \"D\"]\n",
    "        assert self.use_val in [\"best\", \"last\"]\n",
    "\n",
    "        if self.mode == \"S\":\n",
    "            use_pertubation_data = pertubation_data_S if self.use_val == \"last\" else best_pertubation_data_S\n",
    "            loss = loss_S if self.use_val == \"last\" else best_loss_S\n",
    "        elif self.mode == \"D\":\n",
    "            use_pertubation_data = pertubation_data_D if self.use_val == \"last\" else best_pertubation_data_D\n",
    "            loss = loss_D if self.use_val == \"last\" else best_loss_D\n",
    "        \n",
    "        # print(f\"find a better pertubation_{mode} , max val is {self.get_Linfty_norm(use_pertubations)}\")\n",
    "        # print(f\"use_per is :{use_pertubations[2]}\")\n",
    "        return use_pertubation_data, loss\n",
    "\n",
    "    def certi(self, models, adv_x, vae, noise_scheduler, input_ids, weight_dtype=None, target_tensor=None):\n",
    "        unet, text_encoder = models\n",
    "        unet.zero_grad()\n",
    "        text_encoder.zero_grad()\n",
    "        device = torch.device(\"cuda\")\n",
    "\n",
    "        adv_latens = vae.encode(adv_x.to(device, dtype=weight_dtype)).latent_dist.sample()\n",
    "        adv_latens = adv_latens * vae.config.scaling_factor\n",
    "\n",
    "        noise = torch.randn_like(adv_latens)\n",
    "        bsz = adv_latens.shape[0]\n",
    "        timesteps = torch.randint(0, noise_scheduler.config.num_train_timesteps, (bsz,), device=adv_latens.device)\n",
    "        timesteps = timesteps.long()\n",
    "\n",
    "        noisy_latents = noise_scheduler.add_noise(adv_latens, noise, timesteps)\n",
    "        encoder_hidden_states = text_encoder(input_ids.to(device))[0]\n",
    "        model_pred = unet(noisy_latents, timesteps, encoder_hidden_states).sample\n",
    "\n",
    "        if noise_scheduler.config.prediction_type == \"epsilon\":\n",
    "            target = noise\n",
    "        elif noise_scheduler.config.prediction_type == \"v_prediction\":\n",
    "            target = noise_scheduler.get_velocity(adv_latens, noise, timesteps)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown prediction type {noise_scheduler.config.prediction_type}\")\n",
    "\n",
    "        loss = F.mse_loss(model_pred.float(), target.float(), reduction=\"mean\")\n",
    "\n",
    "        if target_tensor is not None:\n",
    "            timesteps = timesteps.to(device)\n",
    "            noisy_latents = noisy_latents.to(device)\n",
    "            xtm1_pred = torch.cat(\n",
    "                [\n",
    "                    noise_scheduler.step(\n",
    "                        model_pred[idx: idx + 1],\n",
    "                        timesteps[idx: idx + 1],\n",
    "                        noisy_latents[idx: idx + 1],\n",
    "                    ).prev_sample\n",
    "                    for idx in range(len(model_pred))\n",
    "                ]\n",
    "            )\n",
    "            xtm1_target = noise_scheduler.add_noise(target_tensor, noise.to(device), (timesteps - 1).to(device))\n",
    "            loss = loss - F.mse_loss(xtm1_pred, xtm1_target)\n",
    "        return loss\n",
    "\n",
    "    def get_loss_D(self, f, adv_image, ori_image, vae, tokenizer, noise_scheduler):\n",
    "        input_ids = tokenizer(\n",
    "            args.instance_prompt,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=tokenizer.model_max_length,\n",
    "            return_tensors=\"pt\",\n",
    "        ).input_ids.repeat(len(adv_image), 1)\n",
    "\n",
    "        loss_P = self.certi(f, adv_image, vae, noise_scheduler, input_ids, weight_dtype=self.weight_dtype)\n",
    "        # 取最大是否合适\n",
    "        pertubation_linf = torch.max(self.get_Linfty_norm(adv_image-ori_image))\n",
    "        loss = - loss_P + (self.lambda_D * torch.abs(pertubation_linf)**self.k)\n",
    "        return loss\n",
    "\n",
    "    def update_pertubation_data_D(self, f, adv_image, ori_image, vae, tokenizer, noise_scheduler):\n",
    "        adv_image.requires_grad = True\n",
    "        loss = self.get_loss_D(f, adv_image, ori_image, vae, tokenizer, noise_scheduler)\n",
    "        loss.backward()\n",
    "        grad_ml_alpha = self.alpha * adv_image.grad.sign()\n",
    "        adv_image_new = adv_image - grad_ml_alpha\n",
    "        adv_image_new = self._clip_(adv_image_new, ori_image, mode = \"D\")\n",
    "        adv_image_new = adv_image_new.detach()\n",
    "        torch.cuda.empty_cache()\n",
    "        return adv_image_new, loss.item()\n",
    "\n",
    "    def update_pertubation_S(self, f, pertubation_data_S, pertubation_data_D, ori_image, vae, tokenizer, noise_scheduler):\n",
    "        # print(f'old pertubation_S: {pertubation_S[2]}')\n",
    "        pertubation_data_S.requires_grad = True\n",
    "        adv_image_S = pertubation_data_S\n",
    "        adv_image_D = pertubation_data_D\n",
    "\n",
    "        input_ids = tokenizer(\n",
    "            args.instance_prompt,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=tokenizer.model_max_length,\n",
    "            return_tensors=\"pt\",\n",
    "        ).input_ids.repeat(len(adv_image_S), 1)\n",
    "\n",
    "        loss_P_S = self.certi(f, adv_image_S, vae, noise_scheduler, input_ids, weight_dtype=self.weight_dtype)\n",
    "        loss_P_D = self.certi(f, adv_image_D, vae, noise_scheduler, input_ids, weight_dtype=self.weight_dtype)\n",
    "\n",
    "        pertubation_linf_S = torch.max(self.get_Linfty_norm(adv_image_S-ori_image))\n",
    "        loss = - loss_P_S + self.lambda_S * (torch.abs(pertubation_linf_S)**self.k) + self.omiga * (torch.abs(loss_P_S - loss_P_D)**self.k)\n",
    "        loss.backward()\n",
    "\n",
    "        # print(f'grad:{self.alpha * pertubation_S.grad.sign()[0]}')\n",
    "        # print(f'now pertubation_S: {pertubation_S[0]}')\n",
    "        grad_ml_alpha = self.alpha * adv_image_S.grad.sign()\n",
    "        # print(f'old pertubation_S: {pertubation_S[2]}')\n",
    "        # print(f' grad_ml_alpha: {grad_ml_alpha[2]}')\n",
    "        adv_image_S_new = adv_image_S - grad_ml_alpha\n",
    "        # print(f'inner:{self.get_Linfty_norm(adv_image_S - grad_ml_alpha-ori_image)}')\n",
    "        # 裁剪到0～255之间,并确保扰动没有超出范围\n",
    "        adv_image_S_new = self._clip_(adv_image_S_new, ori_image, mode='S')\n",
    "        # print(f'new pertubation_S: {pertubation_S_new[2]}')\n",
    "        adv_image_S_new = adv_image_S_new.detach()\n",
    "        # print(f'new pertubation_S: {pertubation_S[2]}')\n",
    "        torch.cuda.empty_cache()\n",
    "        return adv_image_S_new, loss.item()\n",
    "\n",
    "    def get_Linfty_norm(self, images):\n",
    "        abs_images = torch.abs(images)\n",
    "        max_pixels_per_image, _ = torch.max(abs_images, dim=3)\n",
    "        max_pixels_per_image, _ = torch.max(max_pixels_per_image, dim=2)\n",
    "        Linfty_norm, _ = torch.max(max_pixels_per_image, dim=1)\n",
    "        return Linfty_norm\n",
    "\n",
    "    def _clip_(self, adv_x, x, mode):\n",
    "        adv_x = adv_x - x\n",
    "        if self.norm_type == 'l-infty':\n",
    "            if mode == 'S':\n",
    "                adv_x.clamp_(-self.radius, self.radius)\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "        adv_x = adv_x + x\n",
    "        adv_x.clamp_(self.left, self.right)\n",
    "        return adv_x\n",
    "\n",
    "\n",
    "# my_attacker = PAN_attacker(lambda_D = 0.0001, lambda_S = 0.05, alpha = 0.2, omiga = 0.5, k = 2, x_range = [0,255], radius = 11, steps=1, mode = \"D\", use_val = \"last\")\n",
    "my_attacker = PAN_attacker(lambda_D = 0.0001, lambda_S = 0.05, alpha = 1, omiga = 0.5, k = 2, x_range = [0,255], radius = 6, steps=6, mode = \"S\", use_val = \"last\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raise ValueError('wait')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "perturbed_data = deepcopy(original_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.total_trail_num = 4\n",
    "args.total_train_steps = 10\n",
    "args.interval = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 提取保存的中间状态的step数据（0,199，399...）        \n",
    "steps_list = list(init_model_state_pool[0].keys())\n",
    "# 进度条，总train_few_step调用的次数\n",
    "pbar = tqdm(total=args.total_trail_num * num_models * (args.interval // args.advance_steps) * len(steps_list), desc=\"meta poison with model ensemble\")\n",
    "cnt=0\n",
    "# learning perturbation over the ensemble of models\n",
    "# 在多个模型集合上进行扰动优化\n",
    "# 多次实验\n",
    "for _ in range(args.total_trail_num):\n",
    "    # 针对每一个模型\n",
    "    for model_i in range(num_models):\n",
    "        # 确定关键组件\n",
    "        text_encoder, unet, tokenizer, noise_scheduler, vae = MODEL_BANKS[model_i]\n",
    "        # 对于每一个中间状态step\n",
    "        for split_step in steps_list: \n",
    "            # 加载unet和文本编码器的中间状态参数\n",
    "            unet.load_state_dict(init_model_state_pool[model_i][split_step][\"unet\"])\n",
    "            text_encoder.load_state_dict(init_model_state_pool[model_i][split_step][\"text_encoder\"])\n",
    "            f = [unet, text_encoder]\n",
    "            # 每advance_steps步进行一次防御优化\n",
    "            for j in range(args.interval // args.advance_steps):\n",
    "                before = deepcopy(perturbed_data)\n",
    "                perturbed_data,rubust_loss = my_attacker.attack(f, perturbed_data, original_data, vae, tokenizer, noise_scheduler)\n",
    "                print(my_attacker.get_Linfty_norm(perturbed_data.to('cpu')-before.to('cpu')))\n",
    "                print(my_attacker.get_Linfty_norm(perturbed_data.to('cpu')-original_data))\n",
    "                # break\n",
    "                # perturbed_data,rubust_loss = defender.perturb(f, perturbed_data, original_data, vae, tokenizer, noise_scheduler,)\n",
    "                # 扰动优化次数更新 +1\n",
    "                # wandb.log({\"defender_rubust_loss_without_MAT\": rubust_loss})\n",
    "                cnt+=1\n",
    "                \n",
    "                f = train_few_step(\n",
    "                    args,\n",
    "                    f,\n",
    "                    tokenizer,\n",
    "                    noise_scheduler,\n",
    "                    vae,\n",
    "                    perturbed_data.float(),\n",
    "                    args.advance_steps,\n",
    "                    copy_flag = False,\n",
    "                )\n",
    "                pbar.update(1)\n",
    "                # 每1000次扰动优化，保存一次扰动示例图像\n",
    "                if cnt % 1000 == 0:\n",
    "                    save_image(perturbed_data, f\"{cnt}\")\n",
    "            \n",
    "            # frequently release the memory due to limited GPU memory, \n",
    "            # env with more gpu might consider to remove the following lines for boosting speed\n",
    "            # 释放资源\n",
    "            del f \n",
    "            torch.cuda.empty_cache()\n",
    "            # break\n",
    "            \n",
    "        del unet, text_encoder, tokenizer, noise_scheduler, vae\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache() \n",
    "        # break\n",
    "    import gc\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()   \n",
    "    # break   \n",
    "pbar.close()\n",
    "# 保存最后的结果\n",
    "save_image(perturbed_data, \"final\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_image(perturbed_data, \"final\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff = perturbed_data.to('cpu')-original_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_images(diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_images(perturbed_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_attacker.get_Linfty_norm(perturbed_data.to('cpu')-original_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def show_images(perturbed_data):\n",
    "    # 检查 perturbed_data 是否是 4D Tensor\n",
    "    if len(perturbed_data.shape) != 4 or perturbed_data.shape[1] != 3:\n",
    "        raise ValueError(\"Input tensor must have shape [4, 3, 512, 512]\")\n",
    "\n",
    "    # 转换到CPU并转换为numpy数组\n",
    "    images = perturbed_data.cpu().numpy()\n",
    "    \n",
    "    # 创建一个4x1的图像显示框架\n",
    "    fig, axs = plt.subplots(1, 4, figsize=(20, 5))\n",
    "    \n",
    "    for i in range(4):\n",
    "        img = images[i].transpose(1, 2, 0)  # 调整维度为 H x W x C\n",
    "        axs[i].imshow(img.astype('uint8'))  # 确保数据类型为uint8以显示RGB图片\n",
    "        axs[i].axis('off')  # 关闭坐标轴显示\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(perturbed_data[2]>255).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_images(original_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_images(perturbed_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_image(perturbed_data, \"final\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Metacloak",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
