{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 在SD21上测试PAN攻击"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/home/yekai/apps/miniconda/envs/Metacloak/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2024-10-09 17:43:44.794910: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-10-09 17:43:44.795003: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-10-09 17:43:44.796413: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-10-09 17:43:44.805412: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-10-09 17:43:46.063477: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import wandb\n",
    "import argparse\n",
    "import copy\n",
    "import hashlib\n",
    "import itertools\n",
    "import logging\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "import sys\n",
    "repo_path = \"/data/home/yekai/github/mypro/MetaCloak\"\n",
    "sys.path.append(repo_path)\n",
    "from pathlib import Path\n",
    "import datasets\n",
    "import diffusers\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.checkpoint\n",
    "import transformers\n",
    "from accelerate import Accelerator\n",
    "from accelerate.logging import get_logger\n",
    "from accelerate.utils import set_seed\n",
    "from diffusers import AutoencoderKL, DDPMScheduler, DiffusionPipeline, UNet2DConditionModel\n",
    "from diffusers.utils.import_utils import is_xformers_available\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer, PretrainedConfig\n",
    "from robust_facecloak.model.db_train import  DreamBoothDatasetFromTensor\n",
    "from robust_facecloak.model.db_train import import_model_class_from_model_name_or_path\n",
    "from robust_facecloak.generic.data_utils import PromptDataset, load_data\n",
    "from robust_facecloak.generic.share_args import share_parse_args\n",
    "\n",
    "import pickle\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class myargs():\n",
    "    def __init__(self):\n",
    "        self.learning_rate=5e-7\n",
    "        self.total_trail_num = 4\n",
    "        self.instance_prompt=\"a photo of sks person\"\n",
    "        self.class_data_dir=f\"{repo_path}/prior-data/SD21base/class-person\"\n",
    "        self.instance_data_dir_for_adversarial = f\"{repo_path}/dataset/VGGFace2-clean/0/set_B\"\n",
    "        self.output_dir = \"./tmpdata\"\n",
    "        self.class_prompt=\"a photo of a person\"\n",
    "        self.total_train_steps = 1000\n",
    "        self.interval = 200\n",
    "        self.advance_steps = 2\n",
    "        self.radius = 11\n",
    "        self.resolution=512\n",
    "        self.center_crop=True\n",
    "        self.with_prior_preservation=True\n",
    "        self.revision = None\n",
    "        self.prior_loss_weight = 1.0\n",
    "        self.train_text_encoder = True\n",
    "        self.enable_xformers_memory_efficient_attention = True\n",
    "        self.mixed_precision = \"bf16\"\n",
    "        self.attack_pgd_random_start = False\n",
    "        \n",
    "\n",
    "args = myargs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 首先是模型加载和训练代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_few_step(\n",
    "    args,\n",
    "    models,\n",
    "    tokenizer,\n",
    "    noise_scheduler,\n",
    "    vae,\n",
    "    data_tensor: torch.Tensor,\n",
    "    num_steps=20,\n",
    "    step_wise_save=False,\n",
    "    save_step=100, \n",
    "    retain_graph=False,\n",
    "    task_loss_name = None,\n",
    "    copy_flag = True\n",
    "):\n",
    "    # Load the tokenizer\n",
    "    if copy_flag:\n",
    "        unet, text_encoder = copy.deepcopy(models[0]), copy.deepcopy(models[1])\n",
    "    else:\n",
    "        unet, text_encoder = models[0], models[1]\n",
    "    print('copy model done')\n",
    "    # 绑定unet和文本编码器的参数，共同优化\n",
    "    params_to_optimize = itertools.chain(unet.parameters(), text_encoder.parameters())\n",
    "\n",
    "    # 设置优化器，优化目标为unet参数和文本编码器参数\n",
    "    optimizer = torch.optim.AdamW(\n",
    "        params_to_optimize,\n",
    "        lr=args.learning_rate,\n",
    "        betas=(0.9, 0.999),\n",
    "        weight_decay=1e-2,\n",
    "        eps=1e-08,\n",
    "    )\n",
    "\n",
    "    train_dataset = DreamBoothDatasetFromTensor(\n",
    "        data_tensor,\n",
    "        # A photo of sks person\n",
    "        args.instance_prompt,\n",
    "        tokenizer,\n",
    "        args.class_data_dir,\n",
    "        args.class_prompt,\n",
    "        args.resolution,\n",
    "        args.center_crop,\n",
    "    )\n",
    "\n",
    "    weight_dtype = torch.bfloat16\n",
    "    device = torch.device(\"cuda\")\n",
    "\n",
    "    # 将关键模型移动到对应设备\n",
    "    vae.to(device, dtype=weight_dtype)\n",
    "    text_encoder.to(device, dtype=weight_dtype)\n",
    "    unet.to(device, dtype=weight_dtype)\n",
    "\n",
    "    \n",
    "    step2modelstate={}\n",
    "        \n",
    "    pbar = tqdm(total=num_steps, desc=\"training\")\n",
    "    print(\"Start training...\")\n",
    "    for step in range(num_steps):\n",
    "        print(f\"step: {step}/{num_steps}\")\n",
    "        # print(calculate_model_hash(text_encoder))\n",
    "        # 根据设置选择是否保存训练中间过程参数\n",
    "        if step_wise_save and ((step+1) % save_step == 0 or step == 0):\n",
    "            # make sure the model state dict is put to cpu\n",
    "            step2modelstate[step] = {\n",
    "                \"unet\": copy.deepcopy(unet.cpu().state_dict()),\n",
    "                \"text_encoder\": copy.deepcopy(text_encoder.cpu().state_dict()),\n",
    "            }\n",
    "            # move the model back to gpu\n",
    "            unet.to(device, dtype=weight_dtype); text_encoder.to(device, dtype=weight_dtype)\n",
    "            \n",
    "        pbar.update(1)\n",
    "        # 训练模式\n",
    "        unet.train()\n",
    "        text_encoder.train()\n",
    "        # 循环从训练数据集中取一个样本\n",
    "        step_data = train_dataset[step % len(train_dataset)]\n",
    "        # 将样本中的类别图片和实例图片整合并移动到设备上\n",
    "        # print((step_data[\"instance_images\"]))\n",
    "        # print((step_data[\"class_images\"]))\n",
    "        pixel_values = torch.stack([step_data[\"instance_images\"].to(device), step_data[\"class_images\"].to(device)]).to(\n",
    "            device, dtype=weight_dtype\n",
    "        )\n",
    "        # 将样本中的类别提示词和实例提示词整合并移动到设备上\n",
    "        input_ids = torch.cat([step_data[\"instance_prompt_ids\"], step_data[\"class_prompt_ids\"]], dim=0).to(device)\n",
    "        # 使用VAE对图像进行编码，并对潜在表示进行后处理\n",
    "        latents = vae.encode(pixel_values).latent_dist.sample()\n",
    "        latents = latents * vae.config.scaling_factor\n",
    "\n",
    "        # Sample noise that we'll add to the latents\n",
    "        # 向图片编码向量（潜在空间向量表示）添加随机噪声\n",
    "        noise = torch.randn_like(latents)\n",
    "        # batch_size\n",
    "        bsz = latents.shape[0]\n",
    "        # Sample a random timestep for each image\n",
    "        # 为每个图片生成一个随机step\n",
    "        timesteps = torch.randint(0, noise_scheduler.config.num_train_timesteps, (bsz,), device=latents.device)\n",
    "        timesteps = timesteps.long()\n",
    "\n",
    "        # Add noise to the latents according to the noise magnitude at each timestep\n",
    "        # (this is the forward diffusion process)\n",
    "        # 前向过程，得到前向扩散特定时间步后的图片的潜在空间向量\n",
    "        noisy_latents = noise_scheduler.add_noise(latents, noise, timesteps)\n",
    "\n",
    "        # Get the text embedding for conditioning\n",
    "        # 文本编码向量作为条件信息\n",
    "        encoder_hidden_states = text_encoder(input_ids)[0]\n",
    "        \n",
    "        # Predict the noise residual\n",
    "        # 模型基于当前的噪声潜在表示（noisy_latents）、时间步（timesteps）和文本条件（encoder_hidden_states），预测噪声残差\n",
    "        model_pred = unet(noisy_latents, timesteps, encoder_hidden_states).sample\n",
    "\n",
    "        # Get the target for loss depending on the prediction type\n",
    "        # 预测的可以是噪声，也可以是变化速度\n",
    "        if noise_scheduler.config.prediction_type == \"epsilon\":\n",
    "            target = noise\n",
    "        elif noise_scheduler.config.prediction_type == \"v_prediction\":\n",
    "            target = noise_scheduler.get_velocity(latents, noise, timesteps)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown prediction type {noise_scheduler.config.prediction_type}\")\n",
    "\n",
    "        # with prior preservation loss\n",
    "        # 可选是否使用先验保留损失\n",
    "        if args.with_prior_preservation:\n",
    "            # 再次分为一半一半，对应之前的stack操作\n",
    "            model_pred, model_pred_prior = torch.chunk(model_pred, 2, dim=0)\n",
    "            target, target_prior = torch.chunk(target, 2, dim=0)\n",
    "\n",
    "            # Compute instance loss\n",
    "            instance_loss = F.mse_loss(model_pred.float(), target.float(), reduction=\"mean\")\n",
    "\n",
    "            # Compute prior loss  确保在原来类别上的生成能力不丢失\n",
    "            prior_loss = F.mse_loss(model_pred_prior.float(), target_prior.float(), reduction=\"mean\")\n",
    "\n",
    "            # Add the prior loss to the instance loss.\n",
    "            loss = instance_loss + args.prior_loss_weight * prior_loss\n",
    "\n",
    "        else:\n",
    "            # 不使用先验保留损失\n",
    "            loss = F.mse_loss(model_pred.float(), target.float(), reduction=\"mean\")\n",
    "        if task_loss_name is not None:\n",
    "            wandb.log({f\"{task_loss_name}\": loss.item()})\n",
    "        # 反向传播\n",
    "        loss.backward(retain_graph=retain_graph)\n",
    "        # 梯度裁剪\n",
    "        torch.nn.utils.clip_grad_norm_(params_to_optimize, 1.0, error_if_nonfinite=True)\n",
    "        # 参数优化\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    pbar.close()\n",
    "    # 返回训练的参数数据\n",
    "    if step_wise_save:\n",
    "        return [unet, text_encoder], step2modelstate\n",
    "    else:     \n",
    "        return [unet, text_encoder]\n",
    "\n",
    "# 主要模型的加载\n",
    "def load_model(args, model_path):\n",
    "    print(model_path)\n",
    "    # import correct text encoder class\n",
    "    text_encoder_cls = import_model_class_from_model_name_or_path(model_path, args.revision)\n",
    "\n",
    "    # Load scheduler and models\n",
    "    # 文本编码器加载\n",
    "    text_encoder = text_encoder_cls.from_pretrained(\n",
    "        model_path,\n",
    "        subfolder=\"text_encoder\",\n",
    "        revision=args.revision,\n",
    "    )\n",
    "    # unet加载\n",
    "    unet = UNet2DConditionModel.from_pretrained(model_path, subfolder=\"unet\", revision=args.revision)\n",
    "    # tokenizer加载\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        model_path,\n",
    "        subfolder=\"tokenizer\",\n",
    "        revision=args.revision,\n",
    "        use_fast=False,\n",
    "    )\n",
    "    # 使用DDPM同款调度器\n",
    "    noise_scheduler = DDPMScheduler.from_pretrained(model_path, subfolder=\"scheduler\")\n",
    "    # 加载预训练的vae，vae不需要更新参数\n",
    "    vae = AutoencoderKL.from_pretrained(model_path, subfolder=\"vae\", revision=args.revision)\n",
    "\n",
    "    vae.requires_grad_(False)\n",
    "\n",
    "    # 甚至可以不更新文本编码器的参数\n",
    "    if not args.train_text_encoder:\n",
    "        text_encoder.requires_grad_(False)\n",
    "\n",
    "    if args.enable_xformers_memory_efficient_attention:\n",
    "        print(\"You selected to used efficient xformers\")\n",
    "        print(\"Make sure to install the following packages before continue\")\n",
    "        print(\"pip install triton==2.0.0.dev20221031\")\n",
    "        print(\"pip install pip install xformers==0.0.17.dev461\")\n",
    "\n",
    "        unet.enable_xformers_memory_efficient_attention()\n",
    "    # 返回5个关键模型\n",
    "    return text_encoder, unet, tokenizer, noise_scheduler, vae\n",
    "\n",
    "def save_image(perturbed_data, id_stamp):\n",
    "    save_folder = f\"{args.output_dir}/noise-ckpt/{id_stamp}\"\n",
    "    os.makedirs(save_folder, exist_ok=True)\n",
    "    noised_imgs = perturbed_data.detach()\n",
    "    img_names = [\n",
    "        str(instance_path).split(\"/\")[-1]\n",
    "        for instance_path in list(Path(args.instance_data_dir_for_adversarial).iterdir())\n",
    "    ]\n",
    "    for img_pixel, img_name in zip(noised_imgs, img_names):\n",
    "        save_path = os.path.join(save_folder, f\"noisy_{img_name}\")\n",
    "        Image.fromarray(\n",
    "            img_pixel.float().detach().cpu().permute(1, 2, 0).numpy().squeeze().astype(np.uint8)\n",
    "        ).save(save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 加载模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_paths = [\"/data/home/yekai/github/mypro/MetaCloak/SD/stable-diffusion-v1-5\"]\n",
    "num_models = len(model_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a model of type clip_text_model to instantiate a model of type . This is not supported for all configurations of models and can yield errors.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data/home/yekai/github/mypro/MetaCloak/SD/stable-diffusion-v1-5\n",
      "model_path:/data/home/yekai/github/mypro/MetaCloak/SD/stable-diffusion-v1-5\n",
      "You selected to used efficient xformers\n",
      "Make sure to install the following packages before continue\n",
      "pip install triton==2.0.0.dev20221031\n",
      "pip install pip install xformers==0.0.17.dev461\n"
     ]
    }
   ],
   "source": [
    "MODEL_BANKS = [load_model(args, path) for path in model_paths]\n",
    "MODEL_STATEDICTS = [\n",
    "    {\n",
    "        \"text_encoder\": MODEL_BANKS[i][0].state_dict(),\n",
    "        \"unet\": MODEL_BANKS[i][1].state_dict(),\n",
    "    }\n",
    "    for i in range(num_models)\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 开始初始的预训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载原始扰动数据\n",
    "perturbed_data = load_data(\n",
    "    args.instance_data_dir_for_adversarial,\n",
    "    # size=args.resolution,\n",
    "    # center_crop=args.center_crop,\n",
    ")\n",
    "original_data= copy.deepcopy(perturbed_data)\n",
    "\n",
    "\n",
    "init_model_state_pool = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pbar = tqdm(total=num_models, desc=\"initializing models\")\n",
    "# # split sub-models\n",
    "# # 对于每一个模型，都进行一次训练\n",
    "# for j in range(num_models):\n",
    "#     init_model_state_pool[j] = {}\n",
    "#     # 提取关键模块\n",
    "#     text_encoder, unet, tokenizer, noise_scheduler, vae = MODEL_BANKS[j]\n",
    "    \n",
    "#     # 加载unet和text_encoder的模型参数\n",
    "#     unet.load_state_dict(MODEL_STATEDICTS[j][\"unet\"])\n",
    "#     text_encoder.load_state_dict(MODEL_STATEDICTS[j][\"text_encoder\"])\n",
    "#     # 打包unet和text_encoder\n",
    "#     f_ori = [unet, text_encoder]\n",
    "#     # 得到训练total_train_steps步之后的unet, text_encoder参数以及中间状态参数\n",
    "#     print(\"start training model\", j)\n",
    "#     f_ori, step2state_dict = train_few_step(\n",
    "#             args,\n",
    "#             f_ori,\n",
    "#             tokenizer,\n",
    "#             noise_scheduler,\n",
    "#             vae,\n",
    "#             perturbed_data.float(),\n",
    "#             args.total_train_steps,\n",
    "#             step_wise_save=True,\n",
    "#             save_step=args.interval,\n",
    "#             task_loss_name=None,\n",
    "#     )  \n",
    "#     # init_model_state_pool就来保存训练中间状态参数\n",
    "#     init_model_state_pool[j] = step2state_dict\n",
    "\n",
    "#     # 释放占用的资源\n",
    "#     del f_ori, unet, text_encoder, tokenizer, noise_scheduler, vae\n",
    "#     import gc\n",
    "#     gc.collect()\n",
    "#     torch.cuda.empty_cache()\n",
    "#     pbar.update(1)\n",
    "# pbar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAVE init_model_state_pool\n",
    "# 定义保存文件的路径\n",
    "filename = \"./tmpdata/init_model_state_pool_sd2-1.pth\"\n",
    "\n",
    "# 使用pickle将数据保存到文件\n",
    "# with open(filename, 'wb') as file:\n",
    "#     pickle.dump(init_model_state_pool, file)\n",
    "\n",
    "# 读取保存的文件\n",
    "with open(filename, 'rb') as f:\n",
    "    init_model_state_pool = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(init_model_state_pool)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 优化扰动"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 使用求解器的结果"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 使用判别器的结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 该版本无显存溢出问题，但是需要21G显存\n",
    "device = torch.device('cuda')\n",
    "weight_dtype = torch.bfloat16\n",
    "if args.mixed_precision == \"fp32\":\n",
    "    weight_dtype = torch.float32\n",
    "elif args.mixed_precision == \"fp16\":\n",
    "    weight_dtype = torch.float16\n",
    "elif args.mixed_precision == \"bf16\":\n",
    "    weight_dtype = torch.bfloat16\n",
    "# class PAN_attacker():\n",
    "#     def __init__(self, lambda_D=0.1, lambda_S=10, omiga=0.5, alpha=1/255, k=2, radius=11, x_range=[0,255], steps=1, mode = \"D\", use_val = \"last\", no_attack = False):\n",
    "#         self.lambda_D = lambda_D\n",
    "#         self.lambda_S = lambda_S\n",
    "#         self.omiga = omiga\n",
    "#         self.alpha = alpha\n",
    "#         self.k = k\n",
    "#         self.radius = radius\n",
    "#         self.random_start = args.attack_pgd_random_start\n",
    "#         self.weight_dtype = torch.bfloat16  # 默认类型\n",
    "#         self.left = x_range[0]\n",
    "#         self.right = x_range[1]\n",
    "#         self.norm_type = 'l-infty'\n",
    "#         self.steps = steps\n",
    "#         self.mode = mode\n",
    "#         self.use_val = use_val\n",
    "#         self.noattack = no_attack\n",
    "#         if args.mixed_precision == \"fp32\":\n",
    "#             self.weight_dtype = torch.float32\n",
    "#         elif args.mixed_precision == \"fp16\":\n",
    "#             self.weight_dtype = torch.float16\n",
    "#         elif args.mixed_precision == \"bf16\":\n",
    "#             self.weight_dtype = torch.bfloat16\n",
    "        \n",
    "#     def attack(self, f, perturbed_data, ori_image, vae, tokenizer, noise_scheduler):\n",
    "#         if self.noattack:\n",
    "#             print(\"defender no need to defend\")\n",
    "#             return perturbed_data, 0\n",
    "\n",
    "\n",
    "#         f = [f[0].to(device, dtype=self.weight_dtype), f[1].to(device, dtype=self.weight_dtype)]\n",
    "#         vae.to(device, dtype=self.weight_dtype)\n",
    "#         perturbed_data = perturbed_data.to(device)\n",
    "#         # ori_image = deepcopy(perturbed_data).to(device)\n",
    "#         ori_image = ori_image.to(device)\n",
    "#         # batch_size = ori_image.size(0)\n",
    "#         # random start部分操作逻辑未设计\n",
    "#         if self.random_start:\n",
    "#             r=self.radius\n",
    "#             initial_pertubations = torch.zeros_like(ori_image).uniform_(-r, r).to(device)\n",
    "#             adv_image = perturbed_data+initial_pertubations\n",
    "#             perturbed_data = adv_image - self._clip_(adv_image, ori_image, mode=\"D\")\n",
    "#         else:\n",
    "#             initial_pertubations = torch.zeros_like(ori_image).to(device)\n",
    "#         # 此轮攻击的初始扰动都是0\n",
    "#         pertubation_data_D = deepcopy(perturbed_data)\n",
    "#         pertubation_data_S = deepcopy(perturbed_data)\n",
    "#         best_loss_S = float('inf')\n",
    "#         best_loss_D = float('inf')\n",
    "#         best_pertubation_data_S = deepcopy(perturbed_data)\n",
    "#         best_pertubation_data_D = deepcopy(perturbed_data)\n",
    "\n",
    "#         for i in range(self.steps):\n",
    "#             # print(f'step {i} :per_s is {pertubations_S[0]}')\n",
    "#             # 更新扰动D\n",
    "#             pertubation_data_D, loss_D = self.update_pertubation_data_D(f, pertubation_data_D, ori_image, vae, tokenizer, noise_scheduler)\n",
    "#             if loss_D < best_loss_D:\n",
    "#                 best_loss_D = loss_D\n",
    "#                 # if mode == \"D\":\n",
    "#                     # print(f'pertubation_D, max val is {self.get_Linfty_norm(pertubations_D)}')\n",
    "#                     # print(f\"find a better pertubation , max val is {self.get_Linfty_norm( pertubations_D.to('cpu') + perturbed_data.to('cpu') - ori_image.to('cpu') )}\")\n",
    "#                 best_pertubation_data_D = deepcopy(pertubation_data_D)\n",
    "#             # 更新扰动S\n",
    "#             pertubation_data_S, loss_S = self.update_pertubation_S(f, pertubation_data_S, pertubation_data_D, ori_image, vae, tokenizer, noise_scheduler)\n",
    "#             print(f'epoch: {i}, loss_S: {loss_S:.4f}, loss_D: {loss_D: .4f}')\n",
    "#             if loss_S < best_loss_S:\n",
    "#                 best_loss_S = loss_S\n",
    "#                 # if mode == \"S\":\n",
    "#                     # print(f\"find a better pertubation , max val is {self.get_Linfty_norm(pertubations_S.to('cpu') + perturbed_data.to('cpu') - ori_image.to('cpu'))}\")\n",
    "#                 best_pertubation_data_S = deepcopy(pertubation_data_S)\n",
    "        \n",
    "#         assert self.mode in [\"S\", \"D\"]\n",
    "#         assert self.use_val in [\"best\", \"last\"]\n",
    "\n",
    "#         if self.mode == \"S\":\n",
    "#             use_pertubation_data = pertubation_data_S if self.use_val == \"last\" else best_pertubation_data_S\n",
    "#             loss = loss_S if self.use_val == \"last\" else best_loss_S\n",
    "#         elif self.mode == \"D\":\n",
    "#             use_pertubation_data = pertubation_data_D if self.use_val == \"last\" else best_pertubation_data_D\n",
    "#             loss = loss_D if self.use_val == \"last\" else best_loss_D\n",
    "        \n",
    "#         # print(f\"find a better pertubation_{mode} , max val is {self.get_Linfty_norm(use_pertubations)}\")\n",
    "#         # print(f\"use_per is :{use_pertubations[2]}\")\n",
    "#         return use_pertubation_data, loss\n",
    "\n",
    "#     def certi(self, models, adv_x, vae, noise_scheduler, input_ids, weight_dtype=None, target_tensor=None):\n",
    "#         unet, text_encoder = models\n",
    "#         unet.zero_grad()\n",
    "#         text_encoder.zero_grad()\n",
    "#         device = torch.device(\"cuda\")\n",
    "\n",
    "#         adv_latens = vae.encode(adv_x.to(device, dtype=weight_dtype)).latent_dist.sample()\n",
    "#         adv_latens = adv_latens * vae.config.scaling_factor\n",
    "\n",
    "#         noise = torch.randn_like(adv_latens)\n",
    "#         bsz = adv_latens.shape[0]\n",
    "#         timesteps = torch.randint(0, noise_scheduler.config.num_train_timesteps, (bsz,), device=adv_latens.device)\n",
    "#         timesteps = timesteps.long()\n",
    "\n",
    "#         noisy_latents = noise_scheduler.add_noise(adv_latens, noise, timesteps)\n",
    "#         encoder_hidden_states = text_encoder(input_ids.to(device))[0]\n",
    "#         model_pred = unet(noisy_latents, timesteps, encoder_hidden_states).sample\n",
    "\n",
    "#         if noise_scheduler.config.prediction_type == \"epsilon\":\n",
    "#             target = noise\n",
    "#         elif noise_scheduler.config.prediction_type == \"v_prediction\":\n",
    "#             target = noise_scheduler.get_velocity(adv_latens, noise, timesteps)\n",
    "#         else:\n",
    "#             raise ValueError(f\"Unknown prediction type {noise_scheduler.config.prediction_type}\")\n",
    "\n",
    "#         loss = F.mse_loss(model_pred.float(), target.float(), reduction=\"mean\")\n",
    "\n",
    "#         if target_tensor is not None:\n",
    "#             timesteps = timesteps.to(device)\n",
    "#             noisy_latents = noisy_latents.to(device)\n",
    "#             xtm1_pred = torch.cat(\n",
    "#                 [\n",
    "#                     noise_scheduler.step(\n",
    "#                         model_pred[idx: idx + 1],\n",
    "#                         timesteps[idx: idx + 1],\n",
    "#                         noisy_latents[idx: idx + 1],\n",
    "#                     ).prev_sample\n",
    "#                     for idx in range(len(model_pred))\n",
    "#                 ]\n",
    "#             )\n",
    "#             xtm1_target = noise_scheduler.add_noise(target_tensor, noise.to(device), (timesteps - 1).to(device))\n",
    "#             loss = loss - F.mse_loss(xtm1_pred, xtm1_target)\n",
    "#         return loss\n",
    "\n",
    "#     def get_loss_D(self, f, adv_image, ori_image, vae, tokenizer, noise_scheduler):\n",
    "#         input_ids = tokenizer(\n",
    "#             args.instance_prompt,\n",
    "#             truncation=True,\n",
    "#             padding=\"max_length\",\n",
    "#             max_length=tokenizer.model_max_length,\n",
    "#             return_tensors=\"pt\",\n",
    "#         ).input_ids.repeat(len(adv_image), 1)\n",
    "\n",
    "#         loss_P = self.certi(f, adv_image, vae, noise_scheduler, input_ids, weight_dtype=self.weight_dtype)\n",
    "#         # 取最大是否合适\n",
    "#         pertubation_linf = torch.max(self.get_Linfty_norm(adv_image-ori_image))\n",
    "#         loss = - loss_P + (self.lambda_D * torch.abs(pertubation_linf)**self.k)\n",
    "#         return loss\n",
    "\n",
    "#     def update_pertubation_data_D(self, f, adv_image, ori_image, vae, tokenizer, noise_scheduler):\n",
    "#         adv_image.requires_grad = True\n",
    "#         loss = self.get_loss_D(f, adv_image, ori_image, vae, tokenizer, noise_scheduler)\n",
    "#         loss.backward()\n",
    "#         grad_ml_alpha = self.alpha * adv_image.grad.sign()\n",
    "#         adv_image_new = adv_image - grad_ml_alpha\n",
    "#         adv_image_new = self._clip_(adv_image_new, ori_image, mode = \"D\")\n",
    "#         adv_image_new = adv_image_new.detach()\n",
    "#         torch.cuda.empty_cache()\n",
    "#         return adv_image_new, loss.item()\n",
    "\n",
    "#     def update_pertubation_S(self, f, pertubation_data_S, pertubation_data_D, ori_image, vae, tokenizer, noise_scheduler):\n",
    "#         # print(f'old pertubation_S: {pertubation_S[2]}')\n",
    "#         pertubation_data_S.requires_grad = True\n",
    "#         adv_image_S = pertubation_data_S\n",
    "#         adv_image_D = pertubation_data_D\n",
    "\n",
    "#         input_ids = tokenizer(\n",
    "#             args.instance_prompt,\n",
    "#             truncation=True,\n",
    "#             padding=\"max_length\",\n",
    "#             max_length=tokenizer.model_max_length,\n",
    "#             return_tensors=\"pt\",\n",
    "#         ).input_ids.repeat(len(adv_image_S), 1)\n",
    "\n",
    "#         loss_P_S = self.certi(f, adv_image_S, vae, noise_scheduler, input_ids, weight_dtype=self.weight_dtype)\n",
    "#         loss_P_D = self.certi(f, adv_image_D, vae, noise_scheduler, input_ids, weight_dtype=self.weight_dtype)\n",
    "\n",
    "#         pertubation_linf_S = torch.max(self.get_Linfty_norm(adv_image_S-ori_image))\n",
    "#         loss = - loss_P_S + self.lambda_S * (torch.abs(pertubation_linf_S)**self.k) + self.omiga * (torch.abs(loss_P_S - loss_P_D)**self.k)\n",
    "#         loss.backward()\n",
    "\n",
    "#         # print(f'grad:{self.alpha * pertubation_S.grad.sign()[0]}')\n",
    "#         # print(f'now pertubation_S: {pertubation_S[0]}')\n",
    "#         grad_ml_alpha = self.alpha * adv_image_S.grad.sign()\n",
    "#         # print(f'old pertubation_S: {pertubation_S[2]}')\n",
    "#         # print(f' grad_ml_alpha: {grad_ml_alpha[2]}')\n",
    "#         adv_image_S_new = adv_image_S - grad_ml_alpha\n",
    "#         # print(f'inner:{self.get_Linfty_norm(adv_image_S - grad_ml_alpha-ori_image)}')\n",
    "#         # 裁剪到0～255之间,并确保扰动没有超出范围\n",
    "#         adv_image_S_new = self._clip_(adv_image_S_new, ori_image, mode='S')\n",
    "#         # print(f'new pertubation_S: {pertubation_S_new[2]}')\n",
    "#         adv_image_S_new = adv_image_S_new.detach()\n",
    "#         # print(f'new pertubation_S: {pertubation_S[2]}')\n",
    "#         torch.cuda.empty_cache()\n",
    "#         return adv_image_S_new, loss.item()\n",
    "\n",
    "#     def get_Linfty_norm(self, images):\n",
    "#         abs_images = torch.abs(images)\n",
    "#         max_pixels_per_image, _ = torch.max(abs_images, dim=3)\n",
    "#         max_pixels_per_image, _ = torch.max(max_pixels_per_image, dim=2)\n",
    "#         Linfty_norm, _ = torch.max(max_pixels_per_image, dim=1)\n",
    "#         return Linfty_norm\n",
    "\n",
    "#     def _clip_(self, adv_x, x, mode):\n",
    "#         adv_x = adv_x - x\n",
    "#         if self.norm_type == 'l-infty':\n",
    "#             if mode == 'S':\n",
    "#                 adv_x.clamp_(-self.radius, self.radius)\n",
    "#         else:\n",
    "#             raise NotImplementedError\n",
    "#         adv_x = adv_x + x\n",
    "#         adv_x.clamp_(self.left, self.right)\n",
    "#         return adv_x\n",
    "\n",
    "from pan_worker import PANAttacker\n",
    "# my_attacker = PAN_attacker(lambda_D = 0.0001, lambda_S = 0.05, alpha = 0.2, omiga = 0.5, k = 2, x_range = [0,255], radius = 11, steps=1, mode = \"D\", use_val = \"last\")\n",
    "# my_attacker = PAN_attacker(lambda_D = 0.01, lambda_S = 10, alpha = 1, omiga = 0.5, k = 2, x_range = [0,255], radius = 11, steps=6, mode = \"S\", use_val = \"last\")\n",
    "my_attacker = PANAttacker(lambda_D = 0.01, lambda_S = 10, step_size = 1, omiga = 0.5, k = 2, x_range = [0,255], radius = 11, steps=6, mode = \"S\", use_val = \"last\",args=args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raise ValueError('wait')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "perturbed_data = deepcopy(original_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.total_trail_num = 2\n",
    "args.total_train_steps = 10\n",
    "args.interval = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 提取保存的中间状态的step数据（0,199，399...）        \n",
    "steps_list = list(init_model_state_pool[0].keys())\n",
    "# 进度条，总train_few_step调用的次数\n",
    "pbar = tqdm(total=args.total_trail_num * num_models * (args.interval // args.advance_steps) * len(steps_list), desc=\"meta poison with model ensemble\")\n",
    "cnt=0\n",
    "# learning perturbation over the ensemble of models\n",
    "# 在多个模型集合上进行扰动优化\n",
    "# 多次实验\n",
    "for _ in range(args.total_trail_num):\n",
    "    # 针对每一个模型\n",
    "    for model_i in range(num_models):\n",
    "        # 确定关键组件\n",
    "        text_encoder, unet, tokenizer, noise_scheduler, vae = MODEL_BANKS[model_i]\n",
    "        # 对于每一个中间状态step\n",
    "        for split_step in steps_list: \n",
    "            # 加载unet和文本编码器的中间状态参数\n",
    "            unet.load_state_dict(init_model_state_pool[model_i][split_step][\"unet\"])\n",
    "            text_encoder.load_state_dict(init_model_state_pool[model_i][split_step][\"text_encoder\"])\n",
    "            f = [unet, text_encoder]\n",
    "            # 每advance_steps步进行一次防御优化\n",
    "            for j in range(args.interval // args.advance_steps):\n",
    "                before = deepcopy(perturbed_data)\n",
    "                perturbed_data,rubust_loss = my_attacker.attack(f, perturbed_data, original_data, vae, tokenizer, noise_scheduler)\n",
    "                print(my_attacker.get_Linfty_norm(perturbed_data.to('cpu')-before.to('cpu')))\n",
    "                print(my_attacker.get_Linfty_norm(perturbed_data.to('cpu')-original_data))\n",
    "                # break\n",
    "                # perturbed_data,rubust_loss = defender.perturb(f, perturbed_data, original_data, vae, tokenizer, noise_scheduler,)\n",
    "                # 扰动优化次数更新 +1\n",
    "                # wandb.log({\"defender_rubust_loss_without_MAT\": rubust_loss})\n",
    "                cnt+=1\n",
    "                \n",
    "                f = train_few_step(\n",
    "                    args,\n",
    "                    f,\n",
    "                    tokenizer,\n",
    "                    noise_scheduler,\n",
    "                    vae,\n",
    "                    perturbed_data.float(),\n",
    "                    args.advance_steps,\n",
    "                    copy_flag = False,\n",
    "                )\n",
    "                pbar.update(1)\n",
    "                # 每1000次扰动优化，保存一次扰动示例图像\n",
    "                if cnt % 1000 == 0:\n",
    "                    save_image(perturbed_data, f\"{cnt}\")\n",
    "            \n",
    "            # frequently release the memory due to limited GPU memory, \n",
    "            # env with more gpu might consider to remove the following lines for boosting speed\n",
    "            # 释放资源\n",
    "            del f \n",
    "            torch.cuda.empty_cache()\n",
    "            # break\n",
    "            \n",
    "        del unet, text_encoder, tokenizer, noise_scheduler, vae\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache() \n",
    "        # break\n",
    "    import gc\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()   \n",
    "    # break   \n",
    "pbar.close()\n",
    "# 保存最后的结果\n",
    "save_image(perturbed_data, \"final\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "meta poison with model ensemble:   0%|          | 0/12 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using model 0\n",
      "start 1 times of defense optimization in step-0 model\n",
      " adv_image_S.grad.sign(): tensor([[[[ 1.,  1.,  1.,  ...,  1.,  1.,  1.],\n",
      "          [ 1.,  1.,  1.,  ...,  1.,  1.,  1.],\n",
      "          [ 1.,  1.,  1.,  ...,  1.,  1.,  1.],\n",
      "          ...,\n",
      "          [-1., -1.,  1.,  ...,  1.,  1.,  1.],\n",
      "          [-1., -1.,  1.,  ...,  1.,  1.,  1.],\n",
      "          [-1., -1., -1.,  ...,  1.,  1.,  1.]],\n",
      "\n",
      "         [[ 1.,  1.,  1.,  ...,  1.,  1.,  1.],\n",
      "          [ 1.,  1.,  1.,  ...,  1.,  1.,  1.],\n",
      "          [-1., -1., -1.,  ...,  1.,  1.,  1.],\n",
      "          ...,\n",
      "          [-1., -1., -1.,  ...,  1.,  1.,  1.],\n",
      "          [-1., -1., -1.,  ...,  1.,  1.,  1.],\n",
      "          [-1., -1., -1.,  ...,  1.,  1.,  1.]],\n",
      "\n",
      "         [[ 1.,  1.,  1.,  ...,  1.,  1.,  1.],\n",
      "          [ 1.,  1.,  1.,  ...,  1.,  1.,  1.],\n",
      "          [ 1.,  1.,  1.,  ...,  1.,  1.,  1.],\n",
      "          ...,\n",
      "          [-1., -1., -1.,  ...,  1.,  1.,  1.],\n",
      "          [-1., -1., -1.,  ...,  1.,  1.,  1.],\n",
      "          [-1., -1., -1.,  ...,  1.,  1.,  1.]]],\n",
      "\n",
      "\n",
      "        [[[ 1.,  1.,  1.,  ...,  1.,  1.,  1.],\n",
      "          [ 1.,  1.,  1.,  ...,  1.,  1.,  1.],\n",
      "          [ 1.,  1.,  1.,  ...,  1.,  1.,  1.],\n",
      "          ...,\n",
      "          [-1., -1., -1.,  ..., -1., -1., -1.],\n",
      "          [-1., -1., -1.,  ..., -1., -1., -1.],\n",
      "          [-1., -1., -1.,  ..., -1., -1., -1.]],\n",
      "\n",
      "         [[ 1.,  1.,  1.,  ...,  1.,  1.,  1.],\n",
      "          [ 1.,  1.,  1.,  ...,  1.,  1.,  1.],\n",
      "          [ 1.,  1.,  1.,  ...,  1.,  1.,  1.],\n",
      "          ...,\n",
      "          [-1., -1., -1.,  ..., -1., -1., -1.],\n",
      "          [-1., -1., -1.,  ..., -1., -1., -1.],\n",
      "          [-1., -1., -1.,  ..., -1., -1., -1.]],\n",
      "\n",
      "         [[ 1.,  1.,  1.,  ...,  1.,  1.,  1.],\n",
      "          [ 1.,  1.,  1.,  ...,  1.,  1.,  1.],\n",
      "          [ 1.,  1.,  1.,  ...,  1.,  1.,  1.],\n",
      "          ...,\n",
      "          [-1., -1., -1.,  ..., -1., -1., -1.],\n",
      "          [-1., -1., -1.,  ..., -1., -1., -1.],\n",
      "          [-1., -1., -1.,  ..., -1., -1., -1.]]],\n",
      "\n",
      "\n",
      "        [[[ 1.,  1.,  1.,  ..., -1., -1., -1.],\n",
      "          [ 1.,  1.,  1.,  ..., -1., -1., -1.],\n",
      "          [ 1.,  1.,  1.,  ..., -1., -1., -1.],\n",
      "          ...,\n",
      "          [-1., -1., -1.,  ..., -1., -1.,  1.],\n",
      "          [-1., -1., -1.,  ..., -1., -1., -1.],\n",
      "          [ 1.,  1.,  1.,  ..., -1., -1., -1.]],\n",
      "\n",
      "         [[ 1.,  1.,  1.,  ..., -1., -1., -1.],\n",
      "          [ 1.,  1.,  1.,  ..., -1., -1., -1.],\n",
      "          [ 1.,  1.,  1.,  ..., -1., -1., -1.],\n",
      "          ...,\n",
      "          [-1., -1., -1.,  ...,  1.,  1.,  1.],\n",
      "          [-1., -1., -1.,  ...,  1., -1., -1.],\n",
      "          [-1., -1., -1.,  ..., -1., -1., -1.]],\n",
      "\n",
      "         [[ 1.,  1.,  1.,  ..., -1., -1., -1.],\n",
      "          [ 1.,  1.,  1.,  ..., -1., -1., -1.],\n",
      "          [ 1.,  1.,  1.,  ..., -1., -1., -1.],\n",
      "          ...,\n",
      "          [-1., -1., -1.,  ..., -1., -1., -1.],\n",
      "          [-1., -1., -1.,  ..., -1., -1., -1.],\n",
      "          [-1., -1., -1.,  ..., -1., -1., -1.]]],\n",
      "\n",
      "\n",
      "        [[[ 1.,  1.,  1.,  ..., -1., -1., -1.],\n",
      "          [ 1.,  1.,  1.,  ..., -1., -1., -1.],\n",
      "          [ 1.,  1.,  1.,  ..., -1., -1., -1.],\n",
      "          ...,\n",
      "          [ 1.,  1.,  1.,  ...,  1.,  1.,  1.],\n",
      "          [ 1.,  1.,  1.,  ...,  1.,  1.,  1.],\n",
      "          [ 1.,  1.,  1.,  ...,  1.,  1.,  1.]],\n",
      "\n",
      "         [[ 1.,  1.,  1.,  ..., -1., -1., -1.],\n",
      "          [ 1.,  1.,  1.,  ..., -1., -1., -1.],\n",
      "          [ 1.,  1.,  1.,  ..., -1., -1., -1.],\n",
      "          ...,\n",
      "          [ 1.,  1.,  1.,  ...,  1.,  1.,  1.],\n",
      "          [ 1.,  1.,  1.,  ...,  1.,  1.,  1.],\n",
      "          [ 1.,  1.,  1.,  ...,  1.,  1., -1.]],\n",
      "\n",
      "         [[ 1.,  1.,  1.,  ..., -1., -1., -1.],\n",
      "          [ 1.,  1.,  1.,  ..., -1., -1., -1.],\n",
      "          [ 1.,  1.,  1.,  ...,  1.,  1., -1.],\n",
      "          ...,\n",
      "          [ 1., -1.,  1.,  ..., -1., -1., -1.],\n",
      "          [ 1., -1.,  1.,  ..., -1., -1., -1.],\n",
      "          [ 1., -1., -1.,  ..., -1., -1., -1.]]]], device='cuda:0')\n",
      "epoch: 0, loss_S: -0.0192, loss_D: -0.0148\n",
      " adv_image_S.grad.sign(): tensor([[[[-1.,  1.,  1.,  ...,  1.,  1.,  1.],\n",
      "          [ 1.,  1.,  1.,  ...,  1.,  1.,  1.],\n",
      "          [ 1.,  1.,  1.,  ...,  1.,  1.,  1.],\n",
      "          ...,\n",
      "          [ 1.,  1.,  1.,  ...,  1.,  1.,  1.],\n",
      "          [ 1.,  1.,  1.,  ...,  1.,  1.,  1.],\n",
      "          [ 1.,  1.,  1.,  ...,  1.,  1.,  1.]],\n",
      "\n",
      "         [[ 1.,  1.,  1.,  ...,  1.,  1.,  1.],\n",
      "          [ 1.,  1.,  1.,  ...,  1.,  1.,  1.],\n",
      "          [ 1., -1.,  1.,  ...,  1.,  1.,  1.],\n",
      "          ...,\n",
      "          [-1.,  1.,  1.,  ...,  1.,  1.,  1.],\n",
      "          [ 1.,  1.,  1.,  ...,  1.,  1.,  1.],\n",
      "          [ 1.,  1.,  1.,  ...,  1.,  1.,  1.]],\n",
      "\n",
      "         [[ 1.,  1.,  1.,  ...,  1.,  1.,  1.],\n",
      "          [-1.,  1.,  1.,  ...,  1.,  1.,  1.],\n",
      "          [ 1.,  1.,  1.,  ...,  1.,  1.,  1.],\n",
      "          ...,\n",
      "          [-1., -1.,  1.,  ..., -1., -1., -1.],\n",
      "          [ 1.,  1., -1.,  ..., -1., -1., -1.],\n",
      "          [-1.,  1., -1.,  ..., -1., -1., -1.]]],\n",
      "\n",
      "\n",
      "        [[[-1., -1., -1.,  ...,  1.,  1.,  1.],\n",
      "          [-1., -1., -1.,  ...,  1.,  1.,  1.],\n",
      "          [ 1., -1., -1.,  ...,  1.,  1.,  1.],\n",
      "          ...,\n",
      "          [ 1.,  1.,  1.,  ..., -1., -1., -1.],\n",
      "          [ 1.,  1.,  1.,  ..., -1., -1., -1.],\n",
      "          [ 1.,  1.,  1.,  ..., -1., -1., -1.]],\n",
      "\n",
      "         [[-1., -1., -1.,  ...,  1.,  1.,  1.],\n",
      "          [-1., -1., -1.,  ...,  1.,  1.,  1.],\n",
      "          [-1., -1., -1.,  ...,  1.,  1.,  1.],\n",
      "          ...,\n",
      "          [ 1.,  1.,  1.,  ..., -1., -1., -1.],\n",
      "          [ 1.,  1.,  1.,  ..., -1., -1., -1.],\n",
      "          [ 1.,  1.,  1.,  ..., -1., -1., -1.]],\n",
      "\n",
      "         [[-1., -1., -1.,  ..., -1.,  1.,  1.],\n",
      "          [-1., -1., -1.,  ..., -1., -1., -1.],\n",
      "          [-1., -1., -1.,  ..., -1., -1., -1.],\n",
      "          ...,\n",
      "          [ 1.,  1.,  1.,  ...,  1.,  1.,  1.],\n",
      "          [ 1.,  1.,  1.,  ...,  1.,  1.,  1.],\n",
      "          [ 1.,  1.,  1.,  ..., -1.,  1.,  1.]]],\n",
      "\n",
      "\n",
      "        [[[-1., -1., -1.,  ...,  1.,  1.,  1.],\n",
      "          [-1., -1., -1.,  ...,  1.,  1.,  1.],\n",
      "          [-1., -1., -1.,  ...,  1.,  1.,  1.],\n",
      "          ...,\n",
      "          [ 1.,  1.,  1.,  ..., -1., -1., -1.],\n",
      "          [ 1.,  1.,  1.,  ..., -1., -1., -1.],\n",
      "          [ 1.,  1.,  1.,  ..., -1., -1., -1.]],\n",
      "\n",
      "         [[-1., -1., -1.,  ...,  1.,  1.,  1.],\n",
      "          [-1., -1., -1.,  ...,  1.,  1.,  1.],\n",
      "          [-1., -1., -1.,  ...,  1.,  1.,  1.],\n",
      "          ...,\n",
      "          [-1., -1., -1.,  ..., -1., -1., -1.],\n",
      "          [-1., -1., -1.,  ..., -1., -1., -1.],\n",
      "          [-1., -1., -1.,  ..., -1., -1., -1.]],\n",
      "\n",
      "         [[-1., -1., -1.,  ...,  1.,  1.,  1.],\n",
      "          [-1., -1., -1.,  ...,  1.,  1.,  1.],\n",
      "          [-1., -1., -1.,  ...,  1.,  1.,  1.],\n",
      "          ...,\n",
      "          [-1., -1., -1.,  ..., -1., -1., -1.],\n",
      "          [-1., -1., -1.,  ..., -1., -1., -1.],\n",
      "          [-1., -1., -1.,  ..., -1., -1., -1.]]],\n",
      "\n",
      "\n",
      "        [[[-1.,  1.,  1.,  ..., -1.,  1.,  1.],\n",
      "          [ 1.,  1.,  1.,  ..., -1.,  1.,  1.],\n",
      "          [ 1.,  1.,  1.,  ..., -1.,  1.,  1.],\n",
      "          ...,\n",
      "          [ 1.,  1.,  1.,  ..., -1., -1., -1.],\n",
      "          [ 1.,  1.,  1.,  ..., -1., -1., -1.],\n",
      "          [ 1.,  1.,  1.,  ..., -1., -1.,  1.]],\n",
      "\n",
      "         [[-1., -1., -1.,  ..., -1., -1., -1.],\n",
      "          [-1., -1., -1.,  ..., -1., -1., -1.],\n",
      "          [-1., -1.,  1.,  ..., -1., -1., -1.],\n",
      "          ...,\n",
      "          [ 1.,  1.,  1.,  ..., -1., -1., -1.],\n",
      "          [ 1.,  1.,  1.,  ..., -1., -1., -1.],\n",
      "          [ 1.,  1.,  1.,  ..., -1., -1., -1.]],\n",
      "\n",
      "         [[-1., -1., -1.,  ..., -1., -1., -1.],\n",
      "          [-1., -1., -1.,  ..., -1., -1., -1.],\n",
      "          [-1., -1., -1.,  ..., -1., -1., -1.],\n",
      "          ...,\n",
      "          [ 1.,  1.,  1.,  ...,  1.,  1.,  1.],\n",
      "          [ 1.,  1.,  1.,  ...,  1.,  1.,  1.],\n",
      "          [ 1.,  1.,  1.,  ...,  1.,  1.,  1.]]]], device='cuda:0')\n",
      "epoch: 1, loss_S: 9.7769, loss_D: -0.0336\n",
      " adv_image_S.grad.sign(): tensor([[[[-1., -1., -1.,  ...,  1.,  1.,  1.],\n",
      "          [ 1.,  1.,  1.,  ...,  1.,  1.,  1.],\n",
      "          [-1., -1., -1.,  ...,  1., -1., -1.],\n",
      "          ...,\n",
      "          [-1., -1., -1.,  ...,  1.,  1.,  1.],\n",
      "          [-1., -1., -1.,  ...,  1.,  1.,  1.],\n",
      "          [-1., -1., -1.,  ...,  1.,  1.,  1.]],\n",
      "\n",
      "         [[-1., -1., -1.,  ...,  1.,  1.,  1.],\n",
      "          [-1., -1., -1.,  ...,  1.,  1.,  1.],\n",
      "          [-1., -1., -1.,  ...,  1.,  1.,  1.],\n",
      "          ...,\n",
      "          [-1., -1., -1.,  ...,  1.,  1.,  1.],\n",
      "          [-1., -1., -1.,  ...,  1.,  1.,  1.],\n",
      "          [-1., -1., -1.,  ...,  1.,  1., -1.]],\n",
      "\n",
      "         [[ 1.,  1.,  1.,  ...,  1.,  1.,  1.],\n",
      "          [ 1.,  1.,  1.,  ...,  1.,  1.,  1.],\n",
      "          [ 1.,  1.,  1.,  ...,  1.,  1.,  1.],\n",
      "          ...,\n",
      "          [-1., -1., -1.,  ..., -1., -1., -1.],\n",
      "          [-1., -1., -1.,  ..., -1., -1., -1.],\n",
      "          [-1., -1., -1.,  ..., -1., -1., -1.]]],\n",
      "\n",
      "\n",
      "        [[[-1., -1., -1.,  ...,  1.,  1.,  1.],\n",
      "          [-1., -1., -1.,  ...,  1.,  1.,  1.],\n",
      "          [-1., -1., -1.,  ...,  1.,  1.,  1.],\n",
      "          ...,\n",
      "          [ 1.,  1.,  1.,  ...,  1.,  1.,  1.],\n",
      "          [ 1.,  1.,  1.,  ...,  1.,  1.,  1.],\n",
      "          [ 1.,  1.,  1.,  ...,  1.,  1.,  1.]],\n",
      "\n",
      "         [[-1., -1., -1.,  ...,  1.,  1.,  1.],\n",
      "          [-1., -1., -1.,  ...,  1.,  1.,  1.],\n",
      "          [-1., -1., -1.,  ...,  1.,  1.,  1.],\n",
      "          ...,\n",
      "          [-1., -1., -1.,  ...,  1.,  1.,  1.],\n",
      "          [-1., -1.,  1.,  ...,  1.,  1.,  1.],\n",
      "          [-1., -1.,  1.,  ...,  1.,  1.,  1.]],\n",
      "\n",
      "         [[ 1., -1., -1.,  ...,  1.,  1.,  1.],\n",
      "          [ 1.,  1.,  1.,  ...,  1.,  1.,  1.],\n",
      "          [ 1.,  1.,  1.,  ...,  1.,  1.,  1.],\n",
      "          ...,\n",
      "          [-1., -1., -1.,  ...,  1.,  1.,  1.],\n",
      "          [-1., -1., -1.,  ...,  1.,  1.,  1.],\n",
      "          [-1., -1., -1.,  ...,  1.,  1.,  1.]]],\n",
      "\n",
      "\n",
      "        [[[ 1.,  1., -1.,  ..., -1., -1., -1.],\n",
      "          [ 1.,  1., -1.,  ..., -1., -1., -1.],\n",
      "          [-1., -1., -1.,  ..., -1., -1., -1.],\n",
      "          ...,\n",
      "          [-1., -1., -1.,  ...,  1.,  1.,  1.],\n",
      "          [-1., -1., -1.,  ...,  1.,  1.,  1.],\n",
      "          [-1., -1., -1.,  ...,  1.,  1.,  1.]],\n",
      "\n",
      "         [[ 1.,  1.,  1.,  ..., -1., -1., -1.],\n",
      "          [ 1.,  1.,  1.,  ..., -1., -1., -1.],\n",
      "          [ 1.,  1.,  1.,  ..., -1., -1., -1.],\n",
      "          ...,\n",
      "          [ 1.,  1.,  1.,  ...,  1.,  1.,  1.],\n",
      "          [ 1.,  1.,  1.,  ...,  1.,  1.,  1.],\n",
      "          [ 1.,  1.,  1.,  ...,  1.,  1.,  1.]],\n",
      "\n",
      "         [[ 1.,  1.,  1.,  ..., -1., -1., -1.],\n",
      "          [ 1.,  1.,  1.,  ..., -1., -1., -1.],\n",
      "          [ 1.,  1.,  1.,  ..., -1., -1., -1.],\n",
      "          ...,\n",
      "          [ 1.,  1.,  1.,  ...,  1.,  1.,  1.],\n",
      "          [ 1.,  1.,  1.,  ...,  1.,  1.,  1.],\n",
      "          [ 1.,  1.,  1.,  ...,  1.,  1., -1.]]],\n",
      "\n",
      "\n",
      "        [[[ 1., -1.,  1.,  ...,  1.,  1.,  1.],\n",
      "          [ 1.,  1.,  1.,  ...,  1.,  1.,  1.],\n",
      "          [ 1.,  1.,  1.,  ...,  1.,  1.,  1.],\n",
      "          ...,\n",
      "          [-1., -1., -1.,  ...,  1.,  1.,  1.],\n",
      "          [-1., -1., -1.,  ...,  1.,  1.,  1.],\n",
      "          [-1., -1., -1.,  ...,  1.,  1.,  1.]],\n",
      "\n",
      "         [[ 1.,  1.,  1.,  ...,  1.,  1.,  1.],\n",
      "          [ 1.,  1.,  1.,  ...,  1.,  1.,  1.],\n",
      "          [-1., -1.,  1.,  ...,  1.,  1.,  1.],\n",
      "          ...,\n",
      "          [-1., -1., -1.,  ..., -1., -1., -1.],\n",
      "          [-1., -1., -1.,  ..., -1., -1., -1.],\n",
      "          [-1., -1., -1.,  ..., -1., -1.,  1.]],\n",
      "\n",
      "         [[ 1.,  1.,  1.,  ..., -1.,  1.,  1.],\n",
      "          [ 1.,  1.,  1.,  ..., -1., -1.,  1.],\n",
      "          [ 1.,  1.,  1.,  ...,  1.,  1.,  1.],\n",
      "          ...,\n",
      "          [-1., -1., -1.,  ..., -1., -1., -1.],\n",
      "          [-1., -1., -1.,  ..., -1., -1., -1.],\n",
      "          [-1., -1., -1.,  ..., -1., -1., -1.]]]], device='cuda:0')\n",
      "epoch: 2, loss_S: 39.9164, loss_D:  0.0223\n",
      " adv_image_S.grad.sign(): tensor([[[[-1., -1., -1.,  ..., -1., -1., -1.],\n",
      "          [-1., -1., -1.,  ..., -1., -1., -1.],\n",
      "          [-1., -1., -1.,  ..., -1., -1., -1.],\n",
      "          ...,\n",
      "          [ 1.,  1.,  1.,  ..., -1., -1.,  1.],\n",
      "          [ 1.,  1.,  1.,  ..., -1., -1., -1.],\n",
      "          [ 1.,  1.,  1.,  ..., -1., -1., -1.]],\n",
      "\n",
      "         [[-1., -1., -1.,  ...,  1.,  1., -1.],\n",
      "          [-1., -1., -1.,  ...,  1.,  1., -1.],\n",
      "          [-1., -1., -1.,  ...,  1., -1., -1.],\n",
      "          ...,\n",
      "          [ 1.,  1.,  1.,  ..., -1., -1., -1.],\n",
      "          [ 1.,  1.,  1.,  ..., -1., -1., -1.],\n",
      "          [ 1.,  1.,  1.,  ..., -1., -1., -1.]],\n",
      "\n",
      "         [[-1., -1., -1.,  ...,  1.,  1.,  1.],\n",
      "          [-1., -1., -1.,  ...,  1.,  1.,  1.],\n",
      "          [-1., -1., -1.,  ...,  1.,  1.,  1.],\n",
      "          ...,\n",
      "          [-1., -1., -1.,  ..., -1., -1., -1.],\n",
      "          [-1., -1., -1.,  ..., -1., -1., -1.],\n",
      "          [-1., -1., -1.,  ..., -1., -1., -1.]]],\n",
      "\n",
      "\n",
      "        [[[ 1.,  1.,  1.,  ..., -1., -1., -1.],\n",
      "          [ 1.,  1.,  1.,  ..., -1., -1., -1.],\n",
      "          [ 1.,  1.,  1.,  ..., -1., -1., -1.],\n",
      "          ...,\n",
      "          [ 1.,  1.,  1.,  ...,  1.,  1.,  1.],\n",
      "          [ 1.,  1.,  1.,  ...,  1.,  1.,  1.],\n",
      "          [ 1.,  1.,  1.,  ...,  1.,  1.,  1.]],\n",
      "\n",
      "         [[-1., -1., -1.,  ..., -1., -1., -1.],\n",
      "          [-1., -1., -1.,  ..., -1., -1., -1.],\n",
      "          [-1., -1., -1.,  ..., -1., -1., -1.],\n",
      "          ...,\n",
      "          [ 1.,  1.,  1.,  ...,  1.,  1.,  1.],\n",
      "          [ 1.,  1.,  1.,  ...,  1.,  1.,  1.],\n",
      "          [ 1.,  1.,  1.,  ...,  1.,  1.,  1.]],\n",
      "\n",
      "         [[-1., -1., -1.,  ..., -1., -1., -1.],\n",
      "          [-1., -1., -1.,  ..., -1., -1., -1.],\n",
      "          [-1.,  1.,  1.,  ..., -1., -1., -1.],\n",
      "          ...,\n",
      "          [ 1.,  1.,  1.,  ..., -1., -1., -1.],\n",
      "          [ 1.,  1.,  1.,  ...,  1., -1., -1.],\n",
      "          [ 1.,  1.,  1.,  ...,  1., -1., -1.]]],\n",
      "\n",
      "\n",
      "        [[[-1., -1., -1.,  ..., -1., -1., -1.],\n",
      "          [-1., -1., -1.,  ..., -1., -1., -1.],\n",
      "          [-1., -1., -1.,  ..., -1., -1., -1.],\n",
      "          ...,\n",
      "          [-1., -1., -1.,  ...,  1.,  1.,  1.],\n",
      "          [-1., -1., -1.,  ...,  1.,  1.,  1.],\n",
      "          [-1., -1., -1.,  ...,  1.,  1.,  1.]],\n",
      "\n",
      "         [[-1., -1., -1.,  ..., -1., -1., -1.],\n",
      "          [-1., -1., -1.,  ..., -1., -1., -1.],\n",
      "          [-1., -1., -1.,  ..., -1., -1., -1.],\n",
      "          ...,\n",
      "          [-1., -1., -1.,  ...,  1.,  1.,  1.],\n",
      "          [-1., -1., -1.,  ...,  1.,  1.,  1.],\n",
      "          [-1., -1., -1.,  ...,  1.,  1.,  1.]],\n",
      "\n",
      "         [[-1., -1., -1.,  ..., -1., -1., -1.],\n",
      "          [-1., -1., -1.,  ..., -1., -1., -1.],\n",
      "          [-1., -1., -1.,  ..., -1., -1., -1.],\n",
      "          ...,\n",
      "          [-1., -1., -1.,  ..., -1., -1., -1.],\n",
      "          [-1., -1., -1.,  ..., -1., -1., -1.],\n",
      "          [-1., -1., -1.,  ..., -1., -1., -1.]]],\n",
      "\n",
      "\n",
      "        [[[ 1.,  1., -1.,  ..., -1., -1., -1.],\n",
      "          [ 1.,  1.,  1.,  ..., -1., -1., -1.],\n",
      "          [ 1.,  1.,  1.,  ..., -1., -1., -1.],\n",
      "          ...,\n",
      "          [-1., -1., -1.,  ...,  1.,  1.,  1.],\n",
      "          [-1., -1., -1.,  ...,  1.,  1.,  1.],\n",
      "          [-1., -1., -1.,  ...,  1.,  1.,  1.]],\n",
      "\n",
      "         [[ 1.,  1.,  1.,  ..., -1., -1., -1.],\n",
      "          [ 1.,  1.,  1.,  ..., -1., -1., -1.],\n",
      "          [ 1.,  1.,  1.,  ..., -1., -1., -1.],\n",
      "          ...,\n",
      "          [-1., -1., -1.,  ...,  1.,  1.,  1.],\n",
      "          [-1., -1., -1.,  ...,  1.,  1.,  1.],\n",
      "          [-1., -1., -1.,  ...,  1.,  1.,  1.]],\n",
      "\n",
      "         [[ 1.,  1.,  1.,  ..., -1., -1., -1.],\n",
      "          [ 1.,  1.,  1.,  ..., -1., -1., -1.],\n",
      "          [ 1.,  1.,  1.,  ..., -1., -1., -1.],\n",
      "          ...,\n",
      "          [-1., -1., -1.,  ...,  1.,  1.,  1.],\n",
      "          [-1., -1., -1.,  ...,  1.,  1.,  1.],\n",
      "          [-1., -1., -1.,  ...,  1.,  1.,  1.]]]], device='cuda:0')\n",
      "epoch: 3, loss_S: 89.7714, loss_D:  0.0212\n",
      " adv_image_S.grad.sign(): tensor([[[[-1., -1., -1.,  ..., -1., -1., -1.],\n",
      "          [-1., -1., -1.,  ..., -1., -1., -1.],\n",
      "          [-1., -1., -1.,  ..., -1., -1., -1.],\n",
      "          ...,\n",
      "          [ 1.,  1.,  1.,  ...,  1.,  1.,  1.],\n",
      "          [ 1.,  1.,  1.,  ...,  1.,  1.,  1.],\n",
      "          [ 1.,  1.,  1.,  ..., -1.,  1.,  1.]],\n",
      "\n",
      "         [[-1., -1., -1.,  ..., -1., -1., -1.],\n",
      "          [-1., -1., -1.,  ..., -1., -1., -1.],\n",
      "          [-1., -1., -1.,  ..., -1., -1., -1.],\n",
      "          ...,\n",
      "          [ 1.,  1.,  1.,  ...,  1.,  1.,  1.],\n",
      "          [ 1.,  1.,  1.,  ..., -1., -1.,  1.],\n",
      "          [ 1.,  1.,  1.,  ..., -1., -1.,  1.]],\n",
      "\n",
      "         [[-1., -1., -1.,  ..., -1., -1., -1.],\n",
      "          [ 1., -1.,  1.,  ..., -1., -1., -1.],\n",
      "          [ 1.,  1.,  1.,  ..., -1., -1., -1.],\n",
      "          ...,\n",
      "          [ 1.,  1.,  1.,  ...,  1.,  1.,  1.],\n",
      "          [ 1.,  1.,  1.,  ..., -1., -1.,  1.],\n",
      "          [ 1.,  1.,  1.,  ..., -1., -1.,  1.]]],\n",
      "\n",
      "\n",
      "        [[[-1., -1., -1.,  ...,  1.,  1.,  1.],\n",
      "          [-1., -1., -1.,  ...,  1.,  1.,  1.],\n",
      "          [-1., -1., -1.,  ...,  1.,  1.,  1.],\n",
      "          ...,\n",
      "          [ 1.,  1.,  1.,  ...,  1.,  1.,  1.],\n",
      "          [ 1.,  1.,  1.,  ...,  1.,  1.,  1.],\n",
      "          [ 1.,  1.,  1.,  ...,  1.,  1.,  1.]],\n",
      "\n",
      "         [[-1., -1., -1.,  ...,  1.,  1.,  1.],\n",
      "          [-1., -1., -1.,  ...,  1.,  1.,  1.],\n",
      "          [-1., -1., -1.,  ...,  1.,  1.,  1.],\n",
      "          ...,\n",
      "          [-1., -1., -1.,  ..., -1., -1., -1.],\n",
      "          [-1., -1., -1.,  ..., -1., -1., -1.],\n",
      "          [-1., -1., -1.,  ...,  1.,  1., -1.]],\n",
      "\n",
      "         [[-1., -1., -1.,  ...,  1.,  1.,  1.],\n",
      "          [-1., -1., -1.,  ...,  1.,  1.,  1.],\n",
      "          [-1., -1., -1.,  ...,  1.,  1.,  1.],\n",
      "          ...,\n",
      "          [ 1., -1.,  1.,  ..., -1., -1., -1.],\n",
      "          [-1., -1.,  1.,  ..., -1., -1., -1.],\n",
      "          [-1., -1.,  1.,  ..., -1., -1., -1.]]],\n",
      "\n",
      "\n",
      "        [[[ 1.,  1.,  1.,  ...,  1., -1., -1.],\n",
      "          [ 1.,  1.,  1.,  ...,  1., -1., -1.],\n",
      "          [ 1.,  1.,  1.,  ...,  1., -1., -1.],\n",
      "          ...,\n",
      "          [-1., -1., -1.,  ..., -1.,  1.,  1.],\n",
      "          [-1., -1., -1.,  ..., -1., -1.,  1.],\n",
      "          [-1., -1., -1.,  ..., -1., -1., -1.]],\n",
      "\n",
      "         [[ 1.,  1.,  1.,  ..., -1., -1., -1.],\n",
      "          [ 1.,  1.,  1.,  ..., -1., -1., -1.],\n",
      "          [ 1.,  1.,  1.,  ...,  1., -1., -1.],\n",
      "          ...,\n",
      "          [-1., -1., -1.,  ...,  1.,  1.,  1.],\n",
      "          [-1., -1., -1.,  ...,  1.,  1.,  1.],\n",
      "          [-1., -1., -1.,  ..., -1.,  1.,  1.]],\n",
      "\n",
      "         [[ 1.,  1.,  1.,  ...,  1.,  1.,  1.],\n",
      "          [ 1.,  1.,  1.,  ...,  1.,  1.,  1.],\n",
      "          [ 1.,  1.,  1.,  ...,  1.,  1.,  1.],\n",
      "          ...,\n",
      "          [-1., -1., -1.,  ...,  1.,  1.,  1.],\n",
      "          [-1., -1., -1.,  ...,  1.,  1.,  1.],\n",
      "          [-1., -1., -1.,  ...,  1.,  1.,  1.]]],\n",
      "\n",
      "\n",
      "        [[[ 1.,  1.,  1.,  ..., -1., -1., -1.],\n",
      "          [ 1.,  1.,  1.,  ..., -1., -1., -1.],\n",
      "          [ 1.,  1.,  1.,  ...,  1., -1., -1.],\n",
      "          ...,\n",
      "          [-1., -1., -1.,  ...,  1.,  1.,  1.],\n",
      "          [-1., -1., -1.,  ...,  1.,  1.,  1.],\n",
      "          [-1., -1., -1.,  ...,  1.,  1.,  1.]],\n",
      "\n",
      "         [[ 1.,  1.,  1.,  ..., -1., -1., -1.],\n",
      "          [ 1.,  1.,  1.,  ..., -1., -1., -1.],\n",
      "          [ 1.,  1.,  1.,  ..., -1., -1., -1.],\n",
      "          ...,\n",
      "          [-1., -1., -1.,  ...,  1.,  1.,  1.],\n",
      "          [-1., -1., -1.,  ...,  1.,  1.,  1.],\n",
      "          [-1., -1., -1.,  ...,  1.,  1.,  1.]],\n",
      "\n",
      "         [[-1., -1., -1.,  ..., -1., -1., -1.],\n",
      "          [-1., -1., -1.,  ...,  1., -1., -1.],\n",
      "          [ 1., -1., -1.,  ...,  1.,  1., -1.],\n",
      "          ...,\n",
      "          [-1., -1., -1.,  ...,  1.,  1.,  1.],\n",
      "          [-1., -1., -1.,  ...,  1.,  1.,  1.],\n",
      "          [-1., -1., -1.,  ...,  1.,  1., -1.]]]], device='cuda:0')\n",
      "epoch: 4, loss_S: 159.9747, loss_D:  0.1288\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 29\u001b[0m\n\u001b[1;32m     26\u001b[0m mean_delta \u001b[38;5;241m=\u001b[39m perturbed_data\u001b[38;5;241m.\u001b[39mclone()\u001b[38;5;241m.\u001b[39mdetach()\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;66;03m# print(f'sample delta {k}/{args.sampling_times_delta} times')\u001b[39;00m\n\u001b[0;32m---> 29\u001b[0m     perturbed_data,rubust_loss \u001b[38;5;241m=\u001b[39m \u001b[43mmy_attacker\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mperturbed_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moriginal_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvae\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnoise_scheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m     wandb\u001b[38;5;241m.\u001b[39mlog({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mperturbedloss\u001b[39m\u001b[38;5;124m\"\u001b[39m: rubust_loss})\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;66;03m# 此处引入随机梯度朗之万动力学\u001b[39;00m\n",
      "File \u001b[0;32m~/github/mypro/MetaCloak/robust_facecloak/attacks/algs/pan_worker.py:69\u001b[0m, in \u001b[0;36mPANAttacker.attack\u001b[0;34m(self, f, perturbed_data, ori_image, vae, tokenizer, noise_scheduler)\u001b[0m\n\u001b[1;32m     67\u001b[0m     best_pertubation_data_D \u001b[38;5;241m=\u001b[39m deepcopy(pertubation_data_D)\n\u001b[1;32m     68\u001b[0m \u001b[38;5;66;03m# 更新扰动S\u001b[39;00m\n\u001b[0;32m---> 69\u001b[0m pertubation_data_S, loss_S \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate_pertubation_S\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpertubation_data_S\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpertubation_data_D\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mori_image\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvae\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnoise_scheduler\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mepoch: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, loss_S: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss_S\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, loss_D: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss_D\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m .4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m loss_S \u001b[38;5;241m<\u001b[39m best_loss_S:\n",
      "File \u001b[0;32m~/github/mypro/MetaCloak/robust_facecloak/attacks/algs/pan_worker.py:175\u001b[0m, in \u001b[0;36mPANAttacker.update_pertubation_S\u001b[0;34m(self, f, pertubation_data_S, pertubation_data_D, ori_image, vae, tokenizer, noise_scheduler)\u001b[0m\n\u001b[1;32m    165\u001b[0m adv_image_D \u001b[38;5;241m=\u001b[39m pertubation_data_D\n\u001b[1;32m    167\u001b[0m input_ids \u001b[38;5;241m=\u001b[39m tokenizer(\n\u001b[1;32m    168\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39minstance_prompt,\n\u001b[1;32m    169\u001b[0m     truncation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    172\u001b[0m     return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    173\u001b[0m )\u001b[38;5;241m.\u001b[39minput_ids\u001b[38;5;241m.\u001b[39mrepeat(\u001b[38;5;28mlen\u001b[39m(adv_image_S), \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m--> 175\u001b[0m loss_P_S \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcerti\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madv_image_S\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvae\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnoise_scheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight_dtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    176\u001b[0m loss_P_D \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcerti(f, adv_image_D, vae, noise_scheduler, input_ids, weight_dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight_dtype)\n\u001b[1;32m    178\u001b[0m \u001b[38;5;66;03m# \u001b[39;00m\n",
      "File \u001b[0;32m~/github/mypro/MetaCloak/robust_facecloak/attacks/algs/pan_worker.py:107\u001b[0m, in \u001b[0;36mPANAttacker.certi\u001b[0;34m(self, models, adv_x, vae, noise_scheduler, input_ids, weight_dtype, target_tensor)\u001b[0m\n\u001b[1;32m    105\u001b[0m noisy_latents \u001b[38;5;241m=\u001b[39m noise_scheduler\u001b[38;5;241m.\u001b[39madd_noise(adv_latens, noise, timesteps)\n\u001b[1;32m    106\u001b[0m encoder_hidden_states \u001b[38;5;241m=\u001b[39m text_encoder(input_ids\u001b[38;5;241m.\u001b[39mto(device))[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m--> 107\u001b[0m model_pred \u001b[38;5;241m=\u001b[39m \u001b[43munet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnoisy_latents\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimesteps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39msample\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m noise_scheduler\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mprediction_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mepsilon\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    110\u001b[0m     target \u001b[38;5;241m=\u001b[39m noise\n",
      "File \u001b[0;32m~/apps/miniconda/envs/Metacloak/lib/python3.9/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/apps/miniconda/envs/Metacloak/lib/python3.9/site-packages/diffusers/models/unet_2d_condition.py:615\u001b[0m, in \u001b[0;36mUNet2DConditionModel.forward\u001b[0;34m(self, sample, timestep, encoder_hidden_states, class_labels, timestep_cond, attention_mask, cross_attention_kwargs, return_dict)\u001b[0m\n\u001b[1;32m    612\u001b[0m     upsample_size \u001b[38;5;241m=\u001b[39m down_block_res_samples[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m2\u001b[39m:]\n\u001b[1;32m    614\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(upsample_block, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_cross_attention\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m upsample_block\u001b[38;5;241m.\u001b[39mhas_cross_attention:\n\u001b[0;32m--> 615\u001b[0m     sample \u001b[38;5;241m=\u001b[39m \u001b[43mupsample_block\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    616\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    617\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtemb\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43memb\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    618\u001b[0m \u001b[43m        \u001b[49m\u001b[43mres_hidden_states_tuple\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mres_samples\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    619\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    620\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcross_attention_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcross_attention_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    621\u001b[0m \u001b[43m        \u001b[49m\u001b[43mupsample_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mupsample_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    622\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    623\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    624\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    625\u001b[0m     sample \u001b[38;5;241m=\u001b[39m upsample_block(\n\u001b[1;32m    626\u001b[0m         hidden_states\u001b[38;5;241m=\u001b[39msample, temb\u001b[38;5;241m=\u001b[39memb, res_hidden_states_tuple\u001b[38;5;241m=\u001b[39mres_samples, upsample_size\u001b[38;5;241m=\u001b[39mupsample_size\n\u001b[1;32m    627\u001b[0m     )\n",
      "File \u001b[0;32m~/apps/miniconda/envs/Metacloak/lib/python3.9/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/apps/miniconda/envs/Metacloak/lib/python3.9/site-packages/diffusers/models/unet_2d_blocks.py:1813\u001b[0m, in \u001b[0;36mCrossAttnUpBlock2D.forward\u001b[0;34m(self, hidden_states, res_hidden_states_tuple, temb, encoder_hidden_states, cross_attention_kwargs, upsample_size, attention_mask)\u001b[0m\n\u001b[1;32m   1811\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1812\u001b[0m         hidden_states \u001b[38;5;241m=\u001b[39m resnet(hidden_states, temb)\n\u001b[0;32m-> 1813\u001b[0m         hidden_states \u001b[38;5;241m=\u001b[39m \u001b[43mattn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1814\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1815\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1816\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcross_attention_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcross_attention_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1817\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39msample\n\u001b[1;32m   1819\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupsamplers \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1820\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m upsampler \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupsamplers:\n",
      "File \u001b[0;32m~/apps/miniconda/envs/Metacloak/lib/python3.9/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/apps/miniconda/envs/Metacloak/lib/python3.9/site-packages/diffusers/models/transformer_2d.py:265\u001b[0m, in \u001b[0;36mTransformer2DModel.forward\u001b[0;34m(self, hidden_states, encoder_hidden_states, timestep, class_labels, cross_attention_kwargs, return_dict)\u001b[0m\n\u001b[1;32m    263\u001b[0m \u001b[38;5;66;03m# 2. Blocks\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m block \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransformer_blocks:\n\u001b[0;32m--> 265\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[43mblock\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    266\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    267\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimestep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimestep\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcross_attention_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcross_attention_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclass_labels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclass_labels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[38;5;66;03m# 3. Output\u001b[39;00m\n\u001b[1;32m    274\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_input_continuous:\n",
      "File \u001b[0;32m~/apps/miniconda/envs/Metacloak/lib/python3.9/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/apps/miniconda/envs/Metacloak/lib/python3.9/site-packages/diffusers/models/attention.py:307\u001b[0m, in \u001b[0;36mBasicTransformerBlock.forward\u001b[0;34m(self, hidden_states, encoder_hidden_states, timestep, attention_mask, cross_attention_kwargs, class_labels)\u001b[0m\n\u001b[1;32m    302\u001b[0m     norm_hidden_states \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    303\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm2(hidden_states, timestep) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_ada_layer_norm \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm2(hidden_states)\n\u001b[1;32m    304\u001b[0m     )\n\u001b[1;32m    306\u001b[0m     \u001b[38;5;66;03m# 2. Cross-Attention\u001b[39;00m\n\u001b[0;32m--> 307\u001b[0m     attn_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattn2\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    308\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnorm_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    309\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    310\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    311\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcross_attention_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    312\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    313\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m attn_output \u001b[38;5;241m+\u001b[39m hidden_states\n\u001b[1;32m    315\u001b[0m \u001b[38;5;66;03m# 3. Feed-forward\u001b[39;00m\n",
      "File \u001b[0;32m~/apps/miniconda/envs/Metacloak/lib/python3.9/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/apps/miniconda/envs/Metacloak/lib/python3.9/site-packages/diffusers/models/cross_attention.py:205\u001b[0m, in \u001b[0;36mCrossAttention.forward\u001b[0;34m(self, hidden_states, encoder_hidden_states, attention_mask, **cross_attention_kwargs)\u001b[0m\n\u001b[1;32m    201\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, hidden_states, encoder_hidden_states\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, attention_mask\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcross_attention_kwargs):\n\u001b[1;32m    202\u001b[0m     \u001b[38;5;66;03m# The `CrossAttention` class can call different attention processors / attention functions\u001b[39;00m\n\u001b[1;32m    203\u001b[0m     \u001b[38;5;66;03m# here we simply pass along all tensors to the selected processor class\u001b[39;00m\n\u001b[1;32m    204\u001b[0m     \u001b[38;5;66;03m# For standard processors that are defined here, `**cross_attention_kwargs` is empty\u001b[39;00m\n\u001b[0;32m--> 205\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocessor\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    206\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    207\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    208\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    209\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    210\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcross_attention_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    211\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/apps/miniconda/envs/Metacloak/lib/python3.9/site-packages/diffusers/models/cross_attention.py:460\u001b[0m, in \u001b[0;36mXFormersCrossAttnProcessor.__call__\u001b[0;34m(self, attn, hidden_states, encoder_hidden_states, attention_mask)\u001b[0m\n\u001b[1;32m    456\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m xformers\u001b[38;5;241m.\u001b[39mops\u001b[38;5;241m.\u001b[39mmemory_efficient_attention(\n\u001b[1;32m    457\u001b[0m     query, key, value, attn_bias\u001b[38;5;241m=\u001b[39mattention_mask, op\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattention_op\n\u001b[1;32m    458\u001b[0m )\n\u001b[1;32m    459\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m hidden_states\u001b[38;5;241m.\u001b[39mto(query\u001b[38;5;241m.\u001b[39mdtype)\n\u001b[0;32m--> 460\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[43mattn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_to_head_dim\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    462\u001b[0m \u001b[38;5;66;03m# linear proj\u001b[39;00m\n\u001b[1;32m    463\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m attn\u001b[38;5;241m.\u001b[39mto_out[\u001b[38;5;241m0\u001b[39m](hidden_states)\n",
      "File \u001b[0;32m~/apps/miniconda/envs/Metacloak/lib/python3.9/site-packages/diffusers/models/cross_attention.py:217\u001b[0m, in \u001b[0;36mCrossAttention.batch_to_head_dim\u001b[0;34m(self, tensor)\u001b[0m\n\u001b[1;32m    215\u001b[0m batch_size, seq_len, dim \u001b[38;5;241m=\u001b[39m tensor\u001b[38;5;241m.\u001b[39mshape\n\u001b[1;32m    216\u001b[0m tensor \u001b[38;5;241m=\u001b[39m tensor\u001b[38;5;241m.\u001b[39mreshape(batch_size \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m head_size, head_size, seq_len, dim)\n\u001b[0;32m--> 217\u001b[0m tensor \u001b[38;5;241m=\u001b[39m \u001b[43mtensor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpermute\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mhead_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseq_len\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mhead_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    218\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m tensor\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 提取保存的中间状态的step数据（0,199，399...）        \n",
    "steps_list = list(init_model_state_pool[0].keys())\n",
    "# 进度条，总train_few_step调用的次数\n",
    "pbar = tqdm(total=args.total_trail_num * num_models * (args.interval // args.advance_steps) * len(steps_list), desc=\"meta poison with model ensemble\")\n",
    "cnt=0\n",
    "for _ in range(args.total_trail_num):          \n",
    "            # 针对每一个模型\n",
    "            for model_i in range(num_models):\n",
    "                print(f'using model {model_i}')\n",
    "                # 确定关键组件\n",
    "                # start_time = time.time()\n",
    "                text_encoder, unet, tokenizer, noise_scheduler, vae = MODEL_BANKS[model_i]\n",
    "                # 对于每一个中间状态step\n",
    "                for split_step in steps_list: \n",
    "                    # 加载unet和文本编码器的中间状态参数\n",
    "                    unet.load_state_dict(init_model_state_pool[model_i][split_step][\"unet\"])\n",
    "                    text_encoder.load_state_dict(init_model_state_pool[model_i][split_step][\"text_encoder\"])\n",
    "                    f = [unet, text_encoder]\n",
    "                    # f = [unet.to(device_1), text_encoder.to(device_1)]\n",
    "                    \n",
    "                    # 每advance_steps步进行一次防御优化/对于每一组模型参数，进行200/2=100次对抗训练\n",
    "                    print(f'start {args.interval // args.advance_steps} times of defense optimization in step-{split_step} model')\n",
    "                    for j in range(args.interval // args.advance_steps):\n",
    "                        # 更新一次扰动，使得扰动更加强大,后续需要在此处引入随机性（多轮采样优化），并以扰动的平均值作为后续的扰动\n",
    "                        # vkeilo add it\n",
    "                        mean_delta = perturbed_data.clone().detach()\n",
    "                        for k in range(1):\n",
    "                            # print(f'sample delta {k}/{args.sampling_times_delta} times')\n",
    "                            perturbed_data,rubust_loss = my_attacker.attack(f, perturbed_data, original_data, vae, tokenizer, noise_scheduler,)\n",
    "                            wandb.log({\"perturbedloss\": rubust_loss})\n",
    "                            # 此处引入随机梯度朗之万动力学\n",
    "                            mean_delta = args.beta_s * mean_delta + (1 - args.beta_s) * perturbed_data\n",
    "                        mean_delta.detach()\n",
    "                        perturbed_data = mean_delta\n",
    "                        # print(f\"max pixel change:{find_max_pixel_change(perturbed_data, original_data)}\")\n",
    "                        # f[0] = f[0].to(device_0)\n",
    "                        # f[1] = f[1].to(device_0)\n",
    "                        # perturbed_data = defender.perturb(f, perturbed_data, original_data, vae, tokenizer, noise_scheduler)\n",
    "                        \n",
    "                        # 扰动优化次数更新 +1\n",
    "                        cnt+=1\n",
    "                        # 在新的扰动数据下，训练advance_steps步，后续需要在此处引入随机性（多轮采样优化参数），并以参数的平均值作为模型的参数\n",
    "                        back_parameters_list = [f[0].state_dict(),\n",
    "                                                f[1].state_dict()]\n",
    "\n",
    "                        mean_theta_list = [f[0].state_dict(),\n",
    "                                        f[1].state_dict()]\n",
    "                        \n",
    "                        # print(f'start {args.sampling_times_theta} times of theta sampling')\n",
    "                        for k in range(1):\n",
    "                            # print(f'sample theta {k}/{args.sampling_times_theta} times')\n",
    "                            f = train_few_step(\n",
    "                                args,\n",
    "                                f,\n",
    "                                tokenizer,\n",
    "                                noise_scheduler,\n",
    "                                vae,\n",
    "                                perturbed_data.float(),\n",
    "                                args.advance_steps,\n",
    "                                # device = device_1\n",
    "                                dpcopy = False,\n",
    "                                task_loss_name='model_theta_loss',\n",
    "                            )\n",
    "                            torch.cuda.empty_cache()\n",
    "                            for model_index, model in enumerate(f):\n",
    "                                # print(f\"\\nbefore culcu, GPU: {gpu.name}, Free Memory: {gpu.memoryFree / 1024:.2f} GB\")\n",
    "                                for name, p in model.named_parameters():\n",
    "                                    # 先尝试固定学习率的（因为迭代次数暂未确定）\n",
    "                                    # lr_now = lr_scheduler.get_last_lr()[0]\n",
    "                                    # 参数采样,引入随机性\n",
    "                                    # 模型参数也使用指数平均\n",
    "                                    # mean_theta_list[model_index][name] = args.beta_s * mean_theta_list[model_index][name] + (1 - args.beta_s) * p.data.to('cpu')\n",
    "                                    # mean_theta_list[model_index][name] = args.beta_s * mean_theta_list[model_index][name] + (1 - args.beta_s) * p.data\n",
    "                                    mean_theta_list[model_index][name].mul_(args.beta_s).add_((1 - args.beta_s) * p.data)\n",
    "                                # print(f\"\\nafter calcu params, GPU: {gpu.name}, Free Memory: {gpu.memoryFree / 1024:.2f} GB\")\n",
    "                                # torch.cuda.empty_cache()\n",
    "                        # lr_scheduler.step()\n",
    "                        # 对于模型的unet和文本编码器，分别更新参数\n",
    "                        for back_parameters, mean_theta in zip(back_parameters_list,mean_theta_list):\n",
    "                            for name in back_parameters:\n",
    "                                back_parameters[name] = args.beta_p * back_parameters[name] + (1 - args.beta_p) * mean_theta[name]\n",
    "                                # back_parameters[name] = back_parameters[name].float()\n",
    "                                # back_parameters[name].mul_(args.beta_p).add_((1 - args.beta_p) * mean_theta[name])\n",
    "                        for index, model in enumerate(f):\n",
    "                            # model.load_state_dict({k: v.to(device_g) for k, v in back_parameters_list[index].items()})\n",
    "                            model.load_state_dict(back_parameters_list[index])\n",
    "                            pass\n",
    "                        del back_parameters_list\n",
    "                        del mean_theta_list\n",
    "                        gc.collect()\n",
    "                        torch.cuda.empty_cache()\n",
    "                        # f = train_few_step(\n",
    "                        #     args,\n",
    "                        #     f,\n",
    "                        #     tokenizer,\n",
    "                        #     noise_scheduler,\n",
    "                        #     vae,\n",
    "                        #     perturbed_data.float(),\n",
    "                        #     args.advance_steps,\n",
    "                        # )\n",
    "                        pbar.update(1)\n",
    "                        # 每1000次扰动优化，保存一次扰动示例图像\n",
    "                        if cnt % 1000 == 0:\n",
    "                            save_image(perturbed_data, f\"{cnt}\")\n",
    "                    # frequently release the memory due to limited GPU memory, \n",
    "                    # env with more gpu might consider to remove the following lines for boosting speed\n",
    "                    # 释放资源\n",
    "                    del f \n",
    "                    torch.cuda.empty_cache()\n",
    "                # end_time = time.time()\n",
    "                # logger.info(f\"model {model_i} adversarial training Time cost: {(end_time - start_time) / 60} min\")\n",
    "                del unet, text_encoder, tokenizer, noise_scheduler, vae\n",
    "\n",
    "                if torch.cuda.is_available():\n",
    "                    torch.cuda.empty_cache() \n",
    "\n",
    "            import gc\n",
    "            gc.collect()\n",
    "            torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_image(perturbed_data, \"final\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff = perturbed_data.to('cpu')-original_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_images(diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_images(perturbed_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_attacker.get_Linfty_norm(perturbed_data.to('cpu')-original_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def show_images(perturbed_data):\n",
    "    # 检查 perturbed_data 是否是 4D Tensor\n",
    "    if len(perturbed_data.shape) != 4 or perturbed_data.shape[1] != 3:\n",
    "        raise ValueError(\"Input tensor must have shape [4, 3, 512, 512]\")\n",
    "\n",
    "    # 转换到CPU并转换为numpy数组\n",
    "    images = perturbed_data.cpu().numpy()\n",
    "    \n",
    "    # 创建一个4x1的图像显示框架\n",
    "    fig, axs = plt.subplots(1, 4, figsize=(20, 5))\n",
    "    \n",
    "    for i in range(4):\n",
    "        img = images[i].transpose(1, 2, 0)  # 调整维度为 H x W x C\n",
    "        axs[i].imshow(img.astype('uint8'))  # 确保数据类型为uint8以显示RGB图片\n",
    "        axs[i].axis('off')  # 关闭坐标轴显示\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(perturbed_data[2]>255).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_images(original_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_images(perturbed_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_image(perturbed_data, \"final\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Metacloak",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
